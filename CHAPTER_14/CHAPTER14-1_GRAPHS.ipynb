{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3aa21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /opt/homebrew/lib/python3.11/site-packages (5.28.2)\n",
      "Requirement already satisfied: pytz in /opt/homebrew/lib/python3.11/site-packages (from neo4j) (2025.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: rdflib in /opt/homebrew/lib/python3.11/site-packages (7.1.4)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from rdflib) (3.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/keithbourne/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/keithbourne/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/lib/python3.11/site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/keithbourne/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/lib/python3.11/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/homebrew/lib/python3.11/site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.11/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/lib/python3.11/site-packages (0.3.29)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchainhub in /opt/homebrew/lib/python3.11/site-packages (0.1.21)\n",
      "Requirement already satisfied: langchain-community in /opt/homebrew/lib/python3.11/site-packages (0.3.27)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-experimental in /opt/homebrew/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.0.42)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
      "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/lib/python3.11/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/homebrew/lib/python3.11/site-packages (from langchainhub) (2.32.4.20250809)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/homebrew/lib/python3.11/site-packages (from langchain-community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/keithbourne/Library/Python/3.11/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/homebrew/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.7.34)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: requests, openai, langchain-core, langchain-openai, langchain-community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.99.6\n",
      "    Uninstalling openai-1.99.6:\n",
      "      Successfully uninstalled openai-1.99.6\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.74\n",
      "    Uninstalling langchain-core-0.3.74:\n",
      "      Successfully uninstalled langchain-core-0.3.74\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.29\n",
      "    Uninstalling langchain-openai-0.3.29:\n",
      "      Successfully uninstalled langchain-openai-0.3.29\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.3.27\n",
      "    Uninstalling langchain-community-0.3.27:\n",
      "      Successfully uninstalled langchain-community-0.3.27\n",
      "Successfully installed langchain-community-0.3.29 langchain-core-0.3.75 langchain-openai-0.3.32 openai-1.102.0 requests-2.32.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Step 2.3: Install required packages\n",
    "%pip install neo4j\n",
    "%pip install rdflib \n",
    "%pip install pandas\n",
    "%pip install sentence-transformers \n",
    "%pip install faiss-cpu \n",
    "%pip install python-dotenv\n",
    "%pip install -U langchain langchain-openai langchainhub langchain-community langchain-experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5381a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 3.2: Set up imports\n",
    "import os\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "# Load env vars from the file used in previous chapters\n",
    "_ = load_dotenv(dotenv_path='env.txt')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# Neo4j setup\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'neo4j://127.0.0.1:7687')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')\n",
    "NEO4J_PASS =os.getenv('NEO4J_PASS', 'password')\n",
    "\n",
    "# LLM setup\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0.2)\n",
    "# Turn off hosted LangSmith tracing (optional: silences that warning)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0576a469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ontology_nodes.csv and ontology_edges.csv\n",
      "Created ontology_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Load the ontology\n",
    "g = rdflib.Graph()\n",
    "g.parse('FinancialOntology.ttl', format='turtle')\n",
    "\n",
    "# Helper to get first value of a given property\n",
    "def get_first(subject, prop):\n",
    "    for val in g.objects(subject, prop):\n",
    "        return str(val)\n",
    "    return None\n",
    "\n",
    "# --- Collect nodes ---\n",
    "nodes = []\n",
    "for s in g.subjects(RDF.type, OWL.Class):\n",
    "    nodes.append({\n",
    "        'id': str(s),\n",
    "        'label': get_first(s, RDFS.label) or s.split('#')[-1],\n",
    "        'comment': get_first(s, RDFS.comment),\n",
    "        'type': 'Class'\n",
    "    })\n",
    "for s in g.subjects(RDF.type, OWL.NamedIndividual):\n",
    "    nodes.append({\n",
    "        'id': str(s),\n",
    "        'label': get_first(s, RDFS.label) or s.split('#')[-1],\n",
    "        'comment': get_first(s, RDFS.comment),\n",
    "        'type': 'Individual'\n",
    "    })\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "nodes_df.to_csv('ontology_nodes.csv', index=False)\n",
    "\n",
    "# --- Collect edges ---\n",
    "edges = []\n",
    "for s, p, o in g.triples((None, None, None)):\n",
    "    if p in [RDF.type, RDFS.label, RDFS.comment]:\n",
    "        continue\n",
    "    if str(p).startswith('http://www.w3.org/2002/07/owl#'):\n",
    "        continue\n",
    "    if isinstance(o, rdflib.term.Identifier) and str(o).startswith('http'):\n",
    "        edges.append({\n",
    "            'source': str(s),\n",
    "            'target': str(o),\n",
    "            'type': p.split('#')[-1] if '#' in str(p) else str(p).split('/')[-1]\n",
    "        })\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.to_csv('ontology_edges.csv', index=False)\n",
    "\n",
    "print(\"Created ontology_nodes.csv and ontology_edges.csv\")\n",
    "\n",
    "data_rows = []\n",
    "for s, p, o in g.triples((None, None, None)):\n",
    "    # Keep only literal values (data properties)\n",
    "    if isinstance(o, rdflib.term.Literal):\n",
    "        prop_name = p.split('#')[-1] if '#' in str(p) else str(p).rstrip('/').split('/')[-1]\n",
    "        # capture datatype if present\n",
    "        dtype = str(o.datatype) if o.datatype else None\n",
    "        data_rows.append({\n",
    "            'subject': str(s),\n",
    "            'property': prop_name,\n",
    "            'value': str(o),\n",
    "            'datatype': dtype\n",
    "        })\n",
    "\n",
    "pd.DataFrame(data_rows).to_csv('ontology_data.csv', index=False)\n",
    "print(\"Created ontology_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bb3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j at neo4j://127.0.0.1:7687 as user neo4j\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Use credentials loaded in previous cells\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "def run_tx(query, params=None):\n",
    "    with driver.session() as session:\n",
    "        return session.run(query, params or {}).consume()\n",
    "\n",
    "print(f\"Connected to Neo4j at {NEO4J_URI} as user {NEO4J_USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50b78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint ensured: (:Resource {id}) is UNIQUE.\n"
     ]
    }
   ],
   "source": [
    "# Step 4.2: Create schema constraint\n",
    "run_tx(\"\"\"\n",
    "CREATE CONSTRAINT resource_id_unique IF NOT EXISTS\n",
    "FOR (n:Resource) REQUIRE n.id IS UNIQUE\n",
    "\"\"\")\n",
    "print(\"Constraint ensured: (:Resource {id}) is UNIQUE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b615268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  id  \\\n",
      "0  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "1  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "2  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "3  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "4  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "\n",
      "                                               label  comment   type  \n",
      "0  http://www.semanticweb.org/keithbourne/ontolog...      NaN  Class  \n",
      "1                                               Bond      NaN  Class  \n",
      "2  http://www.semanticweb.org/keithbourne/ontolog...      NaN  Class  \n",
      "3  http://www.semanticweb.org/keithbourne/ontolog...      NaN  Class  \n",
      "4  http://www.semanticweb.org/keithbourne/ontolog...      NaN  Class  \n",
      "Imported 17 nodes as :Resource.\n"
     ]
    }
   ],
   "source": [
    "# Step 4.3: Load nodes from ontology_nodes.csv\n",
    "nodes_df = pd.read_csv(\"ontology_nodes.csv\")\n",
    "print(nodes_df.head())\n",
    "\n",
    "# MERGE all nodes as :Resource; store label/comment/type for later use\n",
    "node_query = \"\"\"\n",
    "MERGE (n:Resource {id: $id})\n",
    "SET n.rdfs_label = $rdfs_label,\n",
    "    n.comment    = $comment,\n",
    "    n.kind       = $kind   // 'Class' or 'Individual'\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    for rec in nodes_df.to_dict(orient=\"records\"):\n",
    "        params = {\n",
    "            \"id\": rec[\"id\"],\n",
    "            \"rdfs_label\": rec.get(\"label\"),\n",
    "            \"comment\": rec.get(\"comment\"),\n",
    "            \"kind\": rec.get(\"type\")\n",
    "        }\n",
    "        session.run(node_query, params)\n",
    "print(f\"Imported {len(nodes_df)} nodes as :Resource.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc0d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              source  \\\n",
      "0  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "1  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "2  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "3  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "4  http://www.semanticweb.org/keithbourne/ontolog...   \n",
      "\n",
      "                                              target        type  \n",
      "0  http://www.semanticweb.org/keithbourne/ontolog...    issuedBy  \n",
      "1  http://www.semanticweb.org/keithbourne/ontolog...    issuedBy  \n",
      "2  http://www.semanticweb.org/keithbourne/ontolog...  subClassOf  \n",
      "3  http://www.semanticweb.org/keithbourne/ontolog...      domain  \n",
      "4  http://www.semanticweb.org/keithbourne/ontolog...       range  \n",
      "Imported 5 relationships (ISSUED_BY, IS_REGULATED_BY, OWNED_BY).\n"
     ]
    }
   ],
   "source": [
    "# Step 4.4: Load edges from ontology_edges.csv \n",
    "\n",
    "edges_df = pd.read_csv(\"ontology_edges.csv\")\n",
    "print(edges_df.head())\n",
    "\n",
    "rel_map = {\n",
    "    \"issuedBy\": \"ISSUED_BY\",\n",
    "    \"isRegulatedBy\": \"IS_REGULATED_BY\",\n",
    "    \"ownedBy\": \"OWNED_BY\",\n",
    "}\n",
    "\n",
    "query_issued = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:ISSUED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "query_regulated = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:IS_REGULATED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "query_owned = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:OWNED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    count = 0\n",
    "    for rec in edges_df.to_dict(orient=\"records\"):\n",
    "        t = str(rec.get(\"type\", \"\")).strip()\n",
    "        src = rec.get(\"source\")\n",
    "        tgt = rec.get(\"target\")\n",
    "        if t not in rel_map:\n",
    "            continue  # skip edges we aren't modeling here\n",
    "        if t == \"issuedBy\":\n",
    "            session.run(query_issued, {\"src\": src, \"tgt\": tgt})\n",
    "        elif t == \"isRegulatedBy\":\n",
    "            session.run(query_regulated, {\"src\": src, \"tgt\": tgt})\n",
    "        elif t == \"ownedBy\":\n",
    "            session.run(query_owned, {\"src\": src, \"tgt\": tgt})\n",
    "        count += 1\n",
    "print(f\"Imported {count} relationships (ISSUED_BY, IS_REGULATED_BY, OWNED_BY).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67483fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             subject     property  \\\n",
      "0  http://www.semanticweb.org/keithbourne/ontolog...   definition   \n",
      "1  http://www.semanticweb.org/keithbourne/ontolog...    scopeNote   \n",
      "2  http://www.semanticweb.org/keithbourne/ontolog...     altLabel   \n",
      "3  http://www.semanticweb.org/keithbourne/ontolog...   definition   \n",
      "4  http://www.semanticweb.org/keithbourne/ontolog...  hiddenLabel   \n",
      "\n",
      "                                               value  datatype  \n",
      "0  A security representing equity ownership in a ...       NaN  \n",
      "1  Includes fixed-income securities with defined ...       NaN  \n",
      "2                                      Bond Security       NaN  \n",
      "3  A debt security issued by governments or corpo...       NaN  \n",
      "4                                       equity share       NaN  \n",
      "Set hasTicker on 3 nodes.\n"
     ]
    }
   ],
   "source": [
    "# step 4.5: Load data properties from ontology_data.csv\n",
    "\n",
    "try:\n",
    "    data_df = pd.read_csv(\"ontology_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    data_df = pd.DataFrame(columns=[\"subject\",\"property\",\"value\",\"datatype\"])\n",
    "\n",
    "print(data_df.head())\n",
    "\n",
    "# Only apply properties we care about in this lab (hasTicker)\n",
    "query_has_ticker = \"\"\"\n",
    "MATCH (n:Resource {id: $id})\n",
    "SET n.hasTicker = $val\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    tick_count = 0\n",
    "    for rec in data_df.to_dict(orient=\"records\"):\n",
    "        prop = str(rec.get(\"property\", \"\")).strip()\n",
    "        if prop != \"hasTicker\":\n",
    "            continue\n",
    "        session.run(query_has_ticker, {\"id\": rec.get(\"subject\"), \"val\": rec.get(\"value\")})\n",
    "        tick_count += 1\n",
    "\n",
    "print(f\"Set hasTicker on {tick_count} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01066944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AAPL',\n",
       "  'ticker': 'AAPL',\n",
       "  'id': 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/AAPL'},\n",
       " {'label': 'MSFT',\n",
       "  'ticker': 'MSFT',\n",
       "  'id': 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/MSFT'},\n",
       " {'label': 'USTB',\n",
       "  'ticker': 'USTB',\n",
       "  'id': 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/USTB'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 4.6: Find anything that looks like a stock-ish resource with a ticker\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n:Resource)\n",
    "        WHERE n.hasTicker IS NOT NULL\n",
    "        RETURN n.rdfs_label AS label, n.hasTicker AS ticker, n.id AS id\n",
    "        ORDER BY label\n",
    "    \"\"\")\n",
    "    rows = result.data()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc89efd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5.1: Build rdf:type (class membership) edges from the Turtle file and push to Neo4j\n",
    "type_pairs = []  # (individual_iri, class_iri)\n",
    "\n",
    "# We only want membership of individuals to classes (not \"is a Class\" or \"is a NamedIndividual\")\n",
    "for s, _, o in g.triples((None, RDF.type, None)):\n",
    "    # skip ontology meta (i.e., don't create type edges for the classes themselves)\n",
    "    if o in (OWL.Class, OWL.NamedIndividual):\n",
    "        continue\n",
    "    # many ontologies also mark classes with RDF.type RDFS.Class\n",
    "    if o == RDFS.Class:\n",
    "        continue\n",
    "    # keep only cases where subject looks like an IRI and object looks like a class IRI present in our graph\n",
    "    if isinstance(s, rdflib.term.Identifier) and isinstance(o, rdflib.term.Identifier):\n",
    "        type_pairs.append((str(s), str(o)))\n",
    "\n",
    "len(type_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bc038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5.1 (second code section): Push [:IS_A] edges into Neo4j between the resource (individual) and the class node\n",
    "with driver.session() as session:\n",
    "    q = \"\"\"\n",
    "    MATCH (a:Resource {id: $sid}), (cls:Resource {id: $cid})\n",
    "    MERGE (a)-[:IS_A]->(cls)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for sid, cid in type_pairs:\n",
    "        session.run(q, {\"sid\": sid, \"cid\": cid})\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4f447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors created/updated and INCLUDES relationships established.\n"
     ]
    }
   ],
   "source": [
    "# Step 5.2: Create “All X” concept nodes and wire members\n",
    "\n",
    "anchors = [\n",
    "    {\"name\": \"All Stocks\",  \"class_label\": \"Stock\"},\n",
    "    {\"name\": \"All Bonds\",   \"class_label\": \"Bond\"},\n",
    "    {\"name\": \"All Orgs\",    \"class_label\": \"Organization\"},\n",
    "    {\"name\": \"All Regulators\", \"class_label\": \"RegulatoryAuthority\"},\n",
    "    # add more as your ontology grows\n",
    "]\n",
    "\n",
    "with driver.session() as session:\n",
    "    for a in anchors:\n",
    "        # ensure the anchor concept exists\n",
    "        session.run(\"\"\"\n",
    "            MERGE (c:Concept {name: $name})\n",
    "            ON CREATE SET c.description = $desc\n",
    "        \"\"\", {\n",
    "            \"name\": a[\"name\"],\n",
    "            \"desc\": f\"Anchor node that includes all {a['class_label']} members.\"\n",
    "        })\n",
    "        # clear old INCLUDES from this concept (idempotent refresh)\n",
    "        session.run(\"\"\"\n",
    "            MATCH (c:Concept {name: $name})-[r:INCLUDES]->()\n",
    "            DELETE r\n",
    "        \"\"\", {\"name\": a[\"name\"]})\n",
    "        # connect anchor to all members of the class via IS_A\n",
    "        session.run(\"\"\"\n",
    "            MATCH (c:Concept {name: $name})\n",
    "            MATCH (cls:Resource {rdfs_label: $class_label})\n",
    "            MATCH (n:Resource)-[:IS_A]->(cls)\n",
    "            MERGE (c)-[:INCLUDES]->(n)\n",
    "        \"\"\", {\"name\": a[\"name\"], \"class_label\": a[\"class_label\"]})\n",
    "\n",
    "print(\"Anchors created/updated and INCLUDES relationships established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e9c297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'anchor': 'All Bonds', 'members': 1}, {'anchor': 'All Stocks', 'members': 2}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5.3 (part 1): Count included members per anchor\n",
    "with driver.session() as session:\n",
    "    data = session.run(\"\"\"\n",
    "        MATCH (c:Concept)-[:INCLUDES]->(n:Resource)\n",
    "        RETURN c.name AS anchor, count(n) AS members\n",
    "        ORDER BY anchor\n",
    "    \"\"\").data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d952c293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AAPL', 'ticker': 'AAPL'}, {'label': 'MSFT', 'ticker': 'MSFT'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5.3 (part 2): Peek at a few Stock members\n",
    "with driver.session() as session:\n",
    "    data = session.run(\"\"\"\n",
    "        MATCH (:Concept {name:'All Stocks'})-[:INCLUDES]->(n:Resource)\n",
    "        RETURN n.rdfs_label AS label, n.hasTicker AS ticker\n",
    "        ORDER BY label\n",
    "        LIMIT 10\n",
    "    \"\"\").data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f511e023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>hybridText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Stock AAPL [AAPL] issued by http://www.semanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Stock MSFT [MSFT] issued by http://www.semanti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id class_label  \\\n",
       "0  http://www.semanticweb.org/keithbourne/ontolog...       Stock   \n",
       "1  http://www.semanticweb.org/keithbourne/ontolog...       Stock   \n",
       "\n",
       "                                          hybridText  \n",
       "0  Stock AAPL [AAPL] issued by http://www.semanti...  \n",
       "1  Stock MSFT [MSFT] issued by http://www.semanti...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>hybridText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Bond</td>\n",
       "      <td>Bond USTB [USTB] issued by Victory Capital — NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id class_label  \\\n",
       "0  http://www.semanticweb.org/keithbourne/ontolog...        Bond   \n",
       "\n",
       "                                         hybridText  \n",
       "0  Bond USTB [USTB] issued by Victory Capital — NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6.1: Build enriched hybrid text (with multi-hop)\n",
    "def fetch_hybrid_text_for_class(class_label: str):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with: id, class_label, hybridText\n",
    "    for all :Resource nodes that are IS_A the given class_label,\n",
    "    enriched with multi-hop info.\n",
    "    \"\"\"\n",
    "    cypher = \"\"\"\n",
    "    MATCH (n:Resource)-[:IS_A]->(cls:Resource {rdfs_label: $class_label})\n",
    "    OPTIONAL MATCH (n)-[:ISSUED_BY]->(org:Resource)\n",
    "    OPTIONAL MATCH (n)-[:IS_REGULATED_BY]->(reg:Resource)\n",
    "    OPTIONAL MATCH (org)-[:IS_REGULATED_BY]->(orgreg:Resource)\n",
    "    WITH n, cls, org, reg, orgreg\n",
    "    RETURN\n",
    "      n.id AS id,\n",
    "      cls.rdfs_label AS class_label,\n",
    "      (\n",
    "        coalesce(cls.rdfs_label, '') + ' ' +\n",
    "        coalesce(n.rdfs_label, '') +\n",
    "        CASE WHEN n.hasTicker IS NOT NULL THEN ' [' + n.hasTicker + ']' ELSE '' END +\n",
    "        CASE WHEN org.rdfs_label IS NOT NULL THEN ' issued by ' + org.rdfs_label ELSE '' END +\n",
    "        CASE WHEN reg.rdfs_label IS NOT NULL THEN ' regulated by ' + reg.rdfs_label ELSE '' END +\n",
    "        CASE WHEN n.comment    IS NOT NULL THEN ' — ' + n.comment ELSE '' END +\n",
    "        CASE WHEN orgreg.rdfs_label IS NOT NULL THEN '. Issuer regulated by ' + orgreg.rdfs_label ELSE '' END\n",
    "      ) AS hybridText\n",
    "    ORDER BY n.rdfs_label\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        rows = session.run(cypher, {\"class_label\": class_label}).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example: build for Stocks and Bonds\n",
    "stocks_df = fetch_hybrid_text_for_class(\"Stock\")\n",
    "bonds_df  = fetch_hybrid_text_for_class(\"Bond\")\n",
    "\n",
    "display(stocks_df.head())\n",
    "display(bonds_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d591ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hybrid text entries: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>hybridText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Stock AAPL [AAPL] issued by http://www.semanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock</td>\n",
       "      <td>Stock MSFT [MSFT] issued by http://www.semanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Bond</td>\n",
       "      <td>Bond USTB [USTB] issued by Victory Capital — NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id class_label  \\\n",
       "0  http://www.semanticweb.org/keithbourne/ontolog...       Stock   \n",
       "1  http://www.semanticweb.org/keithbourne/ontolog...       Stock   \n",
       "2  http://www.semanticweb.org/keithbourne/ontolog...        Bond   \n",
       "\n",
       "                                          hybridText  \n",
       "0  Stock AAPL [AAPL] issued by http://www.semanti...  \n",
       "1  Stock MSFT [MSFT] issued by http://www.semanti...  \n",
       "2   Bond USTB [USTB] issued by Victory Capital — NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6.2: Build enriched hybrid text (with multi-hop)\n",
    "orgs_df  = fetch_hybrid_text_for_class(\"Organization\")\n",
    "regs_df  = fetch_hybrid_text_for_class(\"RegulatoryAuthority\")\n",
    "\n",
    "hybrid_df = pd.concat([stocks_df, bonds_df, orgs_df, regs_df], ignore_index=True)\n",
    "print(f\"Total hybrid text entries: {len(hybrid_df)}\")\n",
    "hybrid_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5261a3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3 rows to hybrid_embeddings_input.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 6.3: Export for embedding service\n",
    "out_path = \"hybrid_embeddings_input.csv\"\n",
    "hybrid_df.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {len(hybrid_df)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb799c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.4: embeddings + vectorstore (one place only) ---\n",
    "# local, no-API embedder that returns normalized vectors\n",
    "class STEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.st = SentenceTransformer(model_name)\n",
    "    def embed_documents(self, texts):\n",
    "        return self.st.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return self.st.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "embedder = STEmbeddings()\n",
    "\n",
    "# Build documents from your hybrid_df (from Step 6.1 / 6.2)\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=row.hybridText,\n",
    "        metadata={\"id\": row.id, \"class_label\": row.class_label}\n",
    "    )\n",
    "    for row in hybrid_df.itertuples(index=False)\n",
    "]\n",
    "\n",
    "# Let LangChain build and hold FAISS; no direct `import faiss`\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c312c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5 (updated): semantic search helper + graph expansion using LangChain FAISS\n",
    "def search_hybrid(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use the LangChain FAISS vectorstore to search the hybrid text.\n",
    "    Returns a DataFrame with: id, class_label, hybridText, score\n",
    "    (score is the distance from FAISS; lower is better)\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "    rows = []\n",
    "    for doc, score in results:\n",
    "        rows.append({\n",
    "            \"id\": doc.metadata.get(\"id\"),\n",
    "            \"class_label\": doc.metadata.get(\"class_label\"),\n",
    "            \"hybridText\": doc.page_content,\n",
    "            \"score\": score\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def expand_in_graph(ids, depth: int = 2):\n",
    "    \"\"\"\n",
    "    Given a list of Neo4j node IDs, fetch a small neighborhood.\n",
    "    (Uses the 'driver' from earlier steps.)\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        data = session.run(f\"\"\"\n",
    "            MATCH (n:Resource)\n",
    "            WHERE n.id IN $ids\n",
    "            OPTIONAL MATCH p=(n)-[*1..{depth}]-(m:Resource)\n",
    "            WITH n, collect(distinct m) AS nbrs\n",
    "            RETURN n.id AS id,\n",
    "                   n.rdfs_label AS label,\n",
    "                   n.hasTicker AS ticker,\n",
    "                   n.comment AS comment,\n",
    "                   [x IN nbrs | coalesce(x.rdfs_label, x.id)] AS neighbors\n",
    "            ORDER BY label\n",
    "        \"\"\", {\"ids\": ids}).data()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69161b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: equities regulated by the SEC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>id</th>\n",
       "      <th>hybridText</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock MSFT [MSFT] issued by http://www.semanti...</td>\n",
       "      <td>1.010578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock AAPL [AAPL] issued by http://www.semanti...</td>\n",
       "      <td>1.057379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bond</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Bond USTB [USTB] issued by Victory Capital — NaN</td>\n",
       "      <td>1.634952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_label                                                 id  \\\n",
       "0       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "1       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "2        Bond  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "\n",
       "                                          hybridText     score  \n",
       "0  Stock MSFT [MSFT] issued by http://www.semanti...  1.010578  \n",
       "1  Stock AAPL [AAPL] issued by http://www.semanti...  1.057379  \n",
       "2   Bond USTB [USTB] issued by Victory Capital — NaN  1.634952  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph context (top 3):\n",
      "- AAPL [AAPL] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Apple_Inc', 'Stock', 'MSFT', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- MSFT [MSFT] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Microsoft_Corp', 'Stock', 'AAPL', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- USTB [USTB] :: neighbors=['Victory Capital', 'Bond', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization']\n",
      "\n",
      "Query: Apple stock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>id</th>\n",
       "      <th>hybridText</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock AAPL [AAPL] issued by http://www.semanti...</td>\n",
       "      <td>0.890892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock MSFT [MSFT] issued by http://www.semanti...</td>\n",
       "      <td>1.267464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bond</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Bond USTB [USTB] issued by Victory Capital — NaN</td>\n",
       "      <td>1.641043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_label                                                 id  \\\n",
       "0       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "1       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "2        Bond  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "\n",
       "                                          hybridText     score  \n",
       "0  Stock AAPL [AAPL] issued by http://www.semanti...  0.890892  \n",
       "1  Stock MSFT [MSFT] issued by http://www.semanti...  1.267464  \n",
       "2   Bond USTB [USTB] issued by Victory Capital — NaN  1.641043  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph context (top 3):\n",
      "- AAPL [AAPL] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Apple_Inc', 'Stock', 'MSFT', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- MSFT [MSFT] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Microsoft_Corp', 'Stock', 'AAPL', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- USTB [USTB] :: neighbors=['Victory Capital', 'Bond', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization']\n",
      "\n",
      "Query: government bonds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_label</th>\n",
       "      <th>id</th>\n",
       "      <th>hybridText</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bond</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Bond USTB [USTB] issued by Victory Capital — NaN</td>\n",
       "      <td>1.019285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock AAPL [AAPL] issued by http://www.semanti...</td>\n",
       "      <td>1.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stock</td>\n",
       "      <td>http://www.semanticweb.org/keithbourne/ontolog...</td>\n",
       "      <td>Stock MSFT [MSFT] issued by http://www.semanti...</td>\n",
       "      <td>1.566432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_label                                                 id  \\\n",
       "0        Bond  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "1       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "2       Stock  http://www.semanticweb.org/keithbourne/ontolog...   \n",
       "\n",
       "                                          hybridText     score  \n",
       "0   Bond USTB [USTB] issued by Victory Capital — NaN  1.019285  \n",
       "1  Stock AAPL [AAPL] issued by http://www.semanti...  1.552200  \n",
       "2  Stock MSFT [MSFT] issued by http://www.semanti...  1.566432  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph context (top 3):\n",
      "- AAPL [AAPL] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Apple_Inc', 'Stock', 'MSFT', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- MSFT [MSFT] :: neighbors=['http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Microsoft_Corp', 'Stock', 'AAPL', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority']\n",
      "- USTB [USTB] :: neighbors=['Victory Capital', 'Bond', 'http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization']\n"
     ]
    }
   ],
   "source": [
    "# Step 6.6: Example natural-language queries to test:\n",
    "queries = [\n",
    "    \"equities regulated by the SEC\",\n",
    "    \"Apple stock\",\n",
    "    \"government bonds\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    hits = search_hybrid(q, top_k=5)\n",
    "    display(hits[[\"class_label\",\"id\",\"hybridText\",\"score\"]].head(5))\n",
    "    # Expand top 3 in the graph\n",
    "    top_ids = hits[\"id\"].head(3).tolist()\n",
    "    ctx = expand_in_graph(top_ids, depth=2)\n",
    "    print(\"Graph context (top 3):\")\n",
    "    for row in ctx:\n",
    "        print(f\"- {row['label']} [{row.get('ticker')}] :: neighbors={row['neighbors'][:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5219209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Helper: vector search + graph expansion (LangChain FAISS)\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def vector_search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Semantic search over the LangChain FAISS vectorstore built in Step 6.\n",
    "    Returns a DataFrame with columns: id, class_label, hybridText, score\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "    rows = []\n",
    "    for doc, score in results:\n",
    "        rows.append({\n",
    "            \"id\": doc.metadata.get(\"id\"),\n",
    "            \"class_label\": doc.metadata.get(\"class_label\"),\n",
    "            \"hybridText\": doc.page_content,\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def graph_expand(ids: List[str], depth: int = 2) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each node id, collect a compact neighborhood and key properties.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        data = session.run(f\"\"\"\n",
    "            MATCH (n:Resource)\n",
    "            WHERE n.id IN $ids\n",
    "            OPTIONAL MATCH p=(n)-[*1..{depth}]-(m:Resource)\n",
    "            WITH n, collect(distinct m) AS nbrs\n",
    "            RETURN n.id AS id,\n",
    "                   n.rdfs_label AS label,\n",
    "                   n.hasTicker AS ticker,\n",
    "                   n.comment AS comment,\n",
    "                   [x IN nbrs | coalesce(x.rdfs_label, x.id)] AS neighbors\n",
    "            ORDER BY label\n",
    "        \"\"\", {\"ids\": ids}).data()\n",
    "    return data\n",
    "\n",
    "def build_llm_context(query: str, top_k: int = 5, depth: int = 2, max_neighbors: int = 8) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    End-to-end: vector search -> graph expand -> promptable context block.\n",
    "    Returns dict with 'context', 'hits', and 'citations'.\n",
    "    \"\"\"\n",
    "    hits = vector_search(query, top_k=top_k)\n",
    "\n",
    "    # Expand top hits in the graph\n",
    "    ids = hits[\"id\"].tolist()\n",
    "    expanded = graph_expand(ids, depth=depth)\n",
    "    exp_by_id = {row[\"id\"]: row for row in expanded}\n",
    "\n",
    "    # Build a readable context block with lightweight citations\n",
    "    lines, citations = [], []\n",
    "    for i, row in enumerate(hits.to_dict(orient=\"records\"), start=1):\n",
    "        rid = row[\"id\"]\n",
    "        meta = exp_by_id.get(rid, {})\n",
    "        label = meta.get(\"label\") or rid\n",
    "        ticker = meta.get(\"ticker\")\n",
    "        comment = meta.get(\"comment\")\n",
    "        neighbors = (meta.get(\"neighbors\") or [])[:max_neighbors]\n",
    "        hybrid_text = row.get(\"hybridText\", \"\").strip()\n",
    "\n",
    "        cite = f\"[E{i}]\"\n",
    "        citations.append({\"key\": cite, \"id\": rid, \"label\": label})\n",
    "\n",
    "        entry = f\"{cite} {hybrid_text}\"\n",
    "        if ticker:\n",
    "            entry += f\" (ticker: {ticker})\"\n",
    "        if comment:\n",
    "            entry += f\"\\n    note: {comment}\"\n",
    "        if neighbors:\n",
    "            entry += \"\\n    related: \" + \", \".join(neighbors)\n",
    "        lines.append(entry)\n",
    "\n",
    "    context_header = f\"Query: {query}\\nTop {len(lines)} relevant entities and facts:\\n\"\n",
    "    context_body = \"\\n\\n\".join(lines)\n",
    "    context = context_header + context_body\n",
    "\n",
    "    return {\"context\": context, \"hits\": hits, \"citations\": citations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: equities regulated by the SEC\n",
      "Top 3 relevant entities and facts:\n",
      "[E1] Stock MSFT [MSFT] issued by http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Microsoft_Corp regulated by http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC — NaN (ticker: MSFT)\n",
      "    note: nan\n",
      "    related: http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Microsoft_Corp, Stock, AAPL, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority\n",
      "\n",
      "[E2] Stock AAPL [AAPL] issued by http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Apple_Inc regulated by http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC — NaN (ticker: AAPL)\n",
      "    note: nan\n",
      "    related: http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/SEC, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Apple_Inc, Stock, MSFT, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/RegulatoryAuthority\n",
      "\n",
      "[E3] Bond USTB [USTB] issued by Victory Capital — NaN (ticker: USTB)\n",
      "    note: nan\n",
      "    related: Victory Capital, Bond, http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/Organization\n",
      "\n",
      "Citations:\n",
      "  [E1] -> MSFT (http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/MSFT)\n",
      "  [E2] -> AAPL (http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/AAPL)\n",
      "  [E3] -> USTB (http://www.semanticweb.org/keithbourne/ontologies/2025/7/FinancialOntology/USTB)\n"
     ]
    }
   ],
   "source": [
    "# Step 7.2: Example usage (show output)\n",
    "query = \"equities regulated by the SEC\"\n",
    "bundle = build_llm_context(query, top_k=5, depth=2, max_neighbors=8)\n",
    "\n",
    "print(textwrap.dedent(bundle[\"context\"]))\n",
    "print(\"\\nCitations:\")\n",
    "for c in bundle[\"citations\"]:\n",
    "    print(f\"  {c['key']} -> {c['label']} ({c['id']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa5d799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.3: Return a prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a precise assistant. Use only the context below. \"\n",
    "    \"Cite entities by their bracketed keys (e.g., [E1]).\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer (with citations):\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8897ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query 1: Is AAPL a stock or bond?\n",
      "AAPL is a stock, as indicated by the entity [E1] which describes Stock AAPL [AAPL] issued by Apple Inc.\n",
      "\n",
      "=== Query 2: What type of instrument is USTB?\n",
      "USTB is a bond issued by Victory Capital [E1].\n",
      "\n",
      "=== Query 3: Which authority regulates MSFT?\n",
      "The authority that regulates MSFT is the SEC [E1].\n",
      "\n",
      "=== Query 4: Which equities are regulated by the SEC, and who issues them?\n",
      "The equities regulated by the SEC include:\n",
      "\n",
      "1. Stock MSFT [MSFT] issued by Microsoft Corp [E1].\n",
      "2. Stock AAPL [AAPL] issued by Apple Inc [E2].\n",
      "\n",
      "=== Query 5: What stocks do you know?  What bonds?\n",
      "I know the following stocks and bonds:\n",
      "\n",
      "**Stocks:**\n",
      "1. Stock AAPL [AAPL] issued by [E1] Apple Inc, regulated by [E1] SEC.\n",
      "2. Stock MSFT [MSFT] issued by [E3] Microsoft Corp, regulated by [E3] SEC.\n",
      "\n",
      "**Bonds:**\n",
      "1. Bond USTB [USTB] issued by Victory Capital [E2].\n"
     ]
    }
   ],
   "source": [
    "# Step 8.2: Demo queries\n",
    "def make_context(question: str):\n",
    "    bundle = build_llm_context(question, top_k=5, depth=2, max_neighbors=8)\n",
    "    return {\"context\": bundle[\"context\"], \"question\": question}\n",
    "\n",
    "rag_chain = RunnableLambda(make_context) | chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "user_queries = [\n",
    "   \"Is AAPL a stock or bond?\",\n",
    "   \"What type of instrument is USTB?\",\n",
    "   \"Which authority regulates MSFT?\",\n",
    "   \"Which equities are regulated by the SEC, and who issues them?\",\n",
    "   \"What stocks do you know?  What bonds?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(user_queries, start=1):\n",
    "    print(f\"\\n=== Query {i}: {q}\")\n",
    "    try:\n",
    "        ans = rag_chain.invoke(q)\n",
    "        print(ans)\n",
    "    except Exception as e:\n",
    "        print(f\"[error] {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
