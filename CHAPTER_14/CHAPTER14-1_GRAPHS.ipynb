{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: Install required packages\n",
    "%pip install neo4j\n",
    "%pip install rdflib \n",
    "%pip install pandas\n",
    "%pip install sentence-transformers \n",
    "%pip install faiss-cpu \n",
    "%pip install python-dotenv\n",
    "%pip install -U langchain langchain-openai langchainhub langchain-community langchain-experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5381a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Set up imports\n",
    "import os\n",
    "import rdflib\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import textwrap\n",
    "\n",
    "# Load env vars from the file used in previous chapters\n",
    "_ = load_dotenv(dotenv_path='env.txt')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# Neo4j setup\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'neo4j://127.0.0.1:7687')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')\n",
    "NEO4J_PASS =os.getenv('NEO4J_PASS', 'password')\n",
    "\n",
    "# LLM setup\n",
    "CHAT_MODEL = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=CHAT_MODEL, temperature=0.2)\n",
    "# Turn off hosted LangSmith tracing (optional: silences that warning)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "# reset the Neo4j database (for demo purposes)\n",
    "def reset_neo4j_database():\n",
    "    \"\"\"Remove all nodes and relationships from Neo4j\"\"\"\n",
    "    with driver.session() as session:\n",
    "        # Delete everything in one query\n",
    "        result = session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        summary = result.consume()\n",
    "        print(f\"Neo4j database reset - deleted {summary.counters.nodes_deleted} nodes and {summary.counters.relationships_deleted} relationships\")\n",
    "\n",
    "# Uncomment the line below to reset your database\n",
    "# WARNING: This will delete ALL data in your Neo4j instance\n",
    "reset_neo4j_database()\n",
    "\n",
    "print(f\"Connected to Neo4j at {NEO4J_URI} as user {NEO4J_USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: Load the ontology\n",
    "g = rdflib.Graph()\n",
    "g.parse('FinancialOntology.ttl', format='turtle')\n",
    "\n",
    "# Helper to get first value of a given property\n",
    "def get_first(subject, prop):\n",
    "    for val in g.objects(subject, prop):\n",
    "        return str(val)\n",
    "    return None\n",
    "\n",
    "# --- Collect nodes ---\n",
    "nodes = []\n",
    "for s in g.subjects(RDF.type, OWL.Class):\n",
    "    nodes.append({\n",
    "        'id': str(s),\n",
    "        'label': get_first(s, RDFS.label) or s.split('#')[-1],\n",
    "        'comment': get_first(s, RDFS.comment),\n",
    "        'type': 'Class'\n",
    "    })\n",
    "for s in g.subjects(RDF.type, OWL.NamedIndividual):\n",
    "    nodes.append({\n",
    "        'id': str(s),\n",
    "        'label': get_first(s, RDFS.label) or s.split('#')[-1],\n",
    "        'comment': get_first(s, RDFS.comment),\n",
    "        'type': 'Individual'\n",
    "    })\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "nodes_df.to_csv('ontology_nodes.csv', index=False)\n",
    "\n",
    "# --- Collect edges ---\n",
    "edges = []\n",
    "for s, p, o in g.triples((None, None, None)):\n",
    "    if p in [RDF.type, RDFS.label, RDFS.comment]:\n",
    "        continue\n",
    "    if str(p).startswith('http://www.w3.org/2002/07/owl#'):\n",
    "        continue\n",
    "    if isinstance(o, rdflib.term.Identifier) and str(o).startswith('http'):\n",
    "        edges.append({\n",
    "            'source': str(s),\n",
    "            'target': str(o),\n",
    "            'type': p.split('#')[-1] if '#' in str(p) else str(p).split('/')[-1]\n",
    "        })\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.to_csv('ontology_edges.csv', index=False)\n",
    "\n",
    "print(\"Created ontology_nodes.csv and ontology_edges.csv\")\n",
    "\n",
    "data_rows = []\n",
    "for s, p, o in g.triples((None, None, None)):\n",
    "    # Keep only literal values (data properties)\n",
    "    if isinstance(o, rdflib.term.Literal):\n",
    "        prop_name = p.split('#')[-1] if '#' in str(p) else str(p).rstrip('/').split('/')[-1]\n",
    "        # capture datatype if present\n",
    "        dtype = str(o.datatype) if o.datatype else None\n",
    "        data_rows.append({\n",
    "            'subject': str(s),\n",
    "            'property': prop_name,\n",
    "            'value': str(o),\n",
    "            'datatype': dtype\n",
    "        })\n",
    "\n",
    "pd.DataFrame(data_rows).to_csv('ontology_data.csv', index=False)\n",
    "print(\"Created ontology_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Use credentials loaded in previous cells\n",
    "def run_tx(query, params=None):\n",
    "    with driver.session() as session:\n",
    "        return session.run(query, params or {}).consume()\n",
    "\n",
    "print(f\"Connected to Neo4j at {NEO4J_URI} as user {NEO4J_USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Create schema constraint\n",
    "run_tx(\"\"\"\n",
    "CREATE CONSTRAINT resource_id_unique IF NOT EXISTS\n",
    "FOR (n:Resource) REQUIRE n.id IS UNIQUE\n",
    "\"\"\")\n",
    "print(\"Constraint ensured: (:Resource {id}) is UNIQUE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b615268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.3: Load nodes from ontology_nodes.csv\n",
    "nodes_df = pd.read_csv(\"ontology_nodes.csv\")\n",
    "print(nodes_df.head())\n",
    "\n",
    "# MERGE all nodes as :Resource; store label/comment/type for later use\n",
    "node_query = \"\"\"\n",
    "MERGE (n:Resource {id: $id})\n",
    "SET n.rdfs_label = $rdfs_label,\n",
    "    n.comment    = $comment,\n",
    "    n.kind       = $kind   // 'Class' or 'Individual'\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    for rec in nodes_df.to_dict(orient=\"records\"):\n",
    "        params = {\n",
    "            \"id\": rec[\"id\"],\n",
    "            \"rdfs_label\": rec.get(\"label\"),\n",
    "            \"comment\": rec.get(\"comment\"),\n",
    "            \"kind\": rec.get(\"type\")\n",
    "        }\n",
    "        session.run(node_query, params)\n",
    "print(f\"Imported {len(nodes_df)} nodes as :Resource.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.4: Load edges from ontology_edges.csv \n",
    "\n",
    "edges_df = pd.read_csv(\"ontology_edges.csv\")\n",
    "print(edges_df.head())\n",
    "\n",
    "rel_map = {\n",
    "    \"issuedBy\": \"ISSUED_BY\",\n",
    "    \"isRegulatedBy\": \"IS_REGULATED_BY\",\n",
    "    \"ownedBy\": \"OWNED_BY\",\n",
    "}\n",
    "\n",
    "query_issued = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:ISSUED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "query_regulated = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:IS_REGULATED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "query_owned = \"\"\"\n",
    "MATCH (a:Resource {id: $src}), (b:Resource {id: $tgt})\n",
    "MERGE (a)-[:OWNED_BY]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    count = 0\n",
    "    for rec in edges_df.to_dict(orient=\"records\"):\n",
    "        t = str(rec.get(\"type\", \"\")).strip()\n",
    "        src = rec.get(\"source\")\n",
    "        tgt = rec.get(\"target\")\n",
    "        if t not in rel_map:\n",
    "            continue  # skip edges we aren't modeling here\n",
    "        if t == \"issuedBy\":\n",
    "            session.run(query_issued, {\"src\": src, \"tgt\": tgt})\n",
    "        elif t == \"isRegulatedBy\":\n",
    "            session.run(query_regulated, {\"src\": src, \"tgt\": tgt})\n",
    "        elif t == \"ownedBy\":\n",
    "            session.run(query_owned, {\"src\": src, \"tgt\": tgt})\n",
    "        count += 1\n",
    "print(f\"Imported {count} relationships (ISSUED_BY, IS_REGULATED_BY, OWNED_BY).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67483fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4.5: Load data properties from ontology_data.csv\n",
    "\n",
    "try:\n",
    "    data_df = pd.read_csv(\"ontology_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    data_df = pd.DataFrame(columns=[\"subject\",\"property\",\"value\",\"datatype\"])\n",
    "\n",
    "print(data_df.head())\n",
    "\n",
    "# Only apply properties we care about in this lab (hasTicker)\n",
    "query_has_ticker = \"\"\"\n",
    "MATCH (n:Resource {id: $id})\n",
    "SET n.hasTicker = $val\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    tick_count = 0\n",
    "    for rec in data_df.to_dict(orient=\"records\"):\n",
    "        prop = str(rec.get(\"property\", \"\")).strip()\n",
    "        if prop != \"hasTicker\":\n",
    "            continue\n",
    "        session.run(query_has_ticker, {\"id\": rec.get(\"subject\"), \"val\": rec.get(\"value\")})\n",
    "        tick_count += 1\n",
    "\n",
    "print(f\"Set hasTicker on {tick_count} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01066944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4.6: Find anything that looks like a stock-ish resource with a ticker\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n:Resource)\n",
    "        WHERE n.hasTicker IS NOT NULL\n",
    "        RETURN n.rdfs_label AS label, n.hasTicker AS ticker, n.id AS id\n",
    "        ORDER BY label\n",
    "    \"\"\")\n",
    "    rows = result.data()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Build rdf:type (class membership) edges from the Turtle file and push to Neo4j\n",
    "type_pairs = []  # (individual_iri, class_iri)\n",
    "\n",
    "# We only want membership of individuals to classes (not \"is a Class\" or \"is a NamedIndividual\")\n",
    "for s, _, o in g.triples((None, RDF.type, None)):\n",
    "    # skip ontology meta (i.e., don't create type edges for the classes themselves)\n",
    "    if o in (OWL.Class, OWL.NamedIndividual):\n",
    "        continue\n",
    "    # many ontologies also mark classes with RDF.type RDFS.Class\n",
    "    if o == RDFS.Class:\n",
    "        continue\n",
    "    # keep only cases where subject looks like an IRI and object looks like a class IRI present in our graph\n",
    "    if isinstance(s, rdflib.term.Identifier) and isinstance(o, rdflib.term.Identifier):\n",
    "        type_pairs.append((str(s), str(o)))\n",
    "\n",
    "len(type_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1 (second code section): Push [:IS_A] edges into Neo4j between the resource (individual) and the class node\n",
    "with driver.session() as session:\n",
    "    q = \"\"\"\n",
    "    MATCH (a:Resource {id: $sid}), (cls:Resource {id: $cid})\n",
    "    MERGE (a)-[:IS_A]->(cls)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for sid, cid in type_pairs:\n",
    "        session.run(q, {\"sid\": sid, \"cid\": cid})\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Create “All X” concept nodes and wire members\n",
    "\n",
    "anchors = [\n",
    "    {\"name\": \"All Stocks\",  \"class_label\": \"Stock\"},\n",
    "    {\"name\": \"All Bonds\",   \"class_label\": \"Bond\"},\n",
    "    {\"name\": \"All Orgs\",    \"class_label\": \"Organization\"},\n",
    "    {\"name\": \"All Regulators\", \"class_label\": \"RegulatoryAuthority\"},\n",
    "    # add more as your ontology grows\n",
    "]\n",
    "\n",
    "with driver.session() as session:\n",
    "    for a in anchors:\n",
    "        # ensure the anchor concept exists\n",
    "        session.run(\"\"\"\n",
    "            MERGE (c:Concept {name: $name})\n",
    "            ON CREATE SET c.description = $desc\n",
    "        \"\"\", {\n",
    "            \"name\": a[\"name\"],\n",
    "            \"desc\": f\"Anchor node that includes all {a['class_label']} members.\"\n",
    "        })\n",
    "        # clear old INCLUDES from this concept (idempotent refresh)\n",
    "        session.run(\"\"\"\n",
    "            MATCH (c:Concept {name: $name})-[r:INCLUDES]->()\n",
    "            DELETE r\n",
    "        \"\"\", {\"name\": a[\"name\"]})\n",
    "        # connect anchor to all members of the class via IS_A\n",
    "        session.run(\"\"\"\n",
    "            MATCH (c:Concept {name: $name})\n",
    "            MATCH (cls:Resource {rdfs_label: $class_label})\n",
    "            MATCH (n:Resource)-[:IS_A]->(cls)\n",
    "            MERGE (c)-[:INCLUDES]->(n)\n",
    "        \"\"\", {\"name\": a[\"name\"], \"class_label\": a[\"class_label\"]})\n",
    "\n",
    "print(\"Anchors created/updated and INCLUDES relationships established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3 (part 1): Count included members per anchor\n",
    "with driver.session() as session:\n",
    "    data = session.run(\"\"\"\n",
    "        MATCH (c:Concept)-[:INCLUDES]->(n:Resource)\n",
    "        RETURN c.name AS anchor, count(n) AS members\n",
    "        ORDER BY anchor\n",
    "    \"\"\").data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3 (part 2): Peek at a few Stock members\n",
    "with driver.session() as session:\n",
    "    data = session.run(\"\"\"\n",
    "        MATCH (:Concept {name:'All Stocks'})-[:INCLUDES]->(n:Resource)\n",
    "        RETURN n.rdfs_label AS label, n.hasTicker AS ticker\n",
    "        ORDER BY label\n",
    "        LIMIT 10\n",
    "    \"\"\").data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: Build enriched hybrid text (with multi-hop)\n",
    "def fetch_hybrid_text_for_class(class_label: str):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with: id, class_label, hybridText\n",
    "    for all :Resource nodes that are IS_A the given class_label,\n",
    "    enriched with multi-hop info.\n",
    "    \"\"\"\n",
    "    cypher = \"\"\"\n",
    "    MATCH (n:Resource)-[:IS_A]->(cls:Resource {rdfs_label: $class_label})\n",
    "    OPTIONAL MATCH (n)-[:ISSUED_BY]->(org:Resource)\n",
    "    OPTIONAL MATCH (n)-[:IS_REGULATED_BY]->(reg:Resource)\n",
    "    OPTIONAL MATCH (org)-[:IS_REGULATED_BY]->(orgreg:Resource)\n",
    "    WITH n, cls, org, reg, orgreg\n",
    "    RETURN\n",
    "      n.id AS id,\n",
    "      cls.rdfs_label AS class_label,\n",
    "      (\n",
    "        coalesce(cls.rdfs_label, '') + ' ' +\n",
    "        coalesce(n.rdfs_label, '') +\n",
    "        CASE WHEN n.hasTicker IS NOT NULL THEN ' [' + n.hasTicker + ']' ELSE '' END +\n",
    "        CASE WHEN org.rdfs_label IS NOT NULL THEN ' issued by ' + org.rdfs_label ELSE '' END +\n",
    "        CASE WHEN reg.rdfs_label IS NOT NULL THEN ' regulated by ' + reg.rdfs_label ELSE '' END +\n",
    "        CASE WHEN n.comment    IS NOT NULL THEN ' — ' + n.comment ELSE '' END +\n",
    "        CASE WHEN orgreg.rdfs_label IS NOT NULL THEN '. Issuer regulated by ' + orgreg.rdfs_label ELSE '' END\n",
    "      ) AS hybridText\n",
    "    ORDER BY n.rdfs_label\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        rows = session.run(cypher, {\"class_label\": class_label}).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example: build for Stocks and Bonds\n",
    "stocks_df = fetch_hybrid_text_for_class(\"Stock\")\n",
    "bonds_df  = fetch_hybrid_text_for_class(\"Bond\")\n",
    "\n",
    "display(stocks_df.head())\n",
    "display(bonds_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d591ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Build enriched hybrid text (with multi-hop)\n",
    "orgs_df  = fetch_hybrid_text_for_class(\"Organization\")\n",
    "regs_df  = fetch_hybrid_text_for_class(\"RegulatoryAuthority\")\n",
    "\n",
    "hybrid_df = pd.concat([stocks_df, bonds_df, orgs_df, regs_df], ignore_index=True)\n",
    "print(f\"Total hybrid text entries: {len(hybrid_df)}\")\n",
    "hybrid_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: Export for embedding service\n",
    "out_path = \"hybrid_embeddings_input.csv\"\n",
    "hybrid_df.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {len(hybrid_df)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb799c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.4: embeddings + vectorstore (one place only) ---\n",
    "# local, no-API embedder that returns normalized vectors\n",
    "class STEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.st = SentenceTransformer(model_name)\n",
    "    def embed_documents(self, texts):\n",
    "        return self.st.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return self.st.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "embedder = STEmbeddings()\n",
    "\n",
    "# Build documents from your hybrid_df (from Step 6.1 / 6.2)\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=row.hybridText,\n",
    "        metadata={\"id\": row.id, \"class_label\": row.class_label}\n",
    "    )\n",
    "    for row in hybrid_df.itertuples(index=False)\n",
    "]\n",
    "\n",
    "# Let LangChain build and hold FAISS; no direct `import faiss`\n",
    "vectorstore = FAISS.from_documents(docs, embedder)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5 (updated): semantic search helper + graph expansion using LangChain FAISS\n",
    "def search_hybrid(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use the LangChain FAISS vectorstore to search the hybrid text.\n",
    "    Returns a DataFrame with: id, class_label, hybridText, score\n",
    "    (score is the distance from FAISS; lower is better)\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "    rows = []\n",
    "    for doc, score in results:\n",
    "        rows.append({\n",
    "            \"id\": doc.metadata.get(\"id\"),\n",
    "            \"class_label\": doc.metadata.get(\"class_label\"),\n",
    "            \"hybridText\": doc.page_content,\n",
    "            \"score\": score\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def expand_in_graph(ids, depth: int = 2):\n",
    "    \"\"\"\n",
    "    Given a list of Neo4j node IDs, fetch a small neighborhood.\n",
    "    (Uses the 'driver' from earlier steps.)\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        data = session.run(f\"\"\"\n",
    "            MATCH (n:Resource)\n",
    "            WHERE n.id IN $ids\n",
    "            OPTIONAL MATCH p=(n)-[*1..{depth}]-(m:Resource)\n",
    "            WITH n, collect(distinct m) AS nbrs\n",
    "            RETURN n.id AS id,\n",
    "                   n.rdfs_label AS label,\n",
    "                   n.hasTicker AS ticker,\n",
    "                   n.comment AS comment,\n",
    "                   [x IN nbrs | coalesce(x.rdfs_label, x.id)] AS neighbors\n",
    "            ORDER BY label\n",
    "        \"\"\", {\"ids\": ids}).data()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69161b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.6: Example natural-language queries to test:\n",
    "queries = [\n",
    "    \"equities regulated by the SEC\",\n",
    "    \"Apple stock\",\n",
    "    \"government bonds\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    hits = search_hybrid(q, top_k=5)\n",
    "    display(hits[[\"class_label\",\"id\",\"hybridText\",\"score\"]].head(5))\n",
    "    # Expand top 3 in the graph\n",
    "    top_ids = hits[\"id\"].head(3).tolist()\n",
    "    ctx = expand_in_graph(top_ids, depth=2)\n",
    "    print(\"Graph context (top 3):\")\n",
    "    for row in ctx:\n",
    "        print(f\"- {row['label']} [{row.get('ticker')}] :: neighbors={row['neighbors'][:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5219209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Helper: vector search + graph expansion (LangChain FAISS)\n",
    "def vector_search(query: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Semantic search over the LangChain FAISS vectorstore built in Step 6.\n",
    "    Returns a DataFrame with columns: id, class_label, hybridText, score\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "    rows = []\n",
    "    for doc, score in results:\n",
    "        rows.append({\n",
    "            \"id\": doc.metadata.get(\"id\"),\n",
    "            \"class_label\": doc.metadata.get(\"class_label\"),\n",
    "            \"hybridText\": doc.page_content,\n",
    "            \"score\": float(score),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def graph_expand(ids: List[str], depth: int = 2) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each node id, collect a compact neighborhood and key properties.\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        data = session.run(f\"\"\"\n",
    "            MATCH (n:Resource)\n",
    "            WHERE n.id IN $ids\n",
    "            OPTIONAL MATCH p=(n)-[*1..{depth}]-(m:Resource)\n",
    "            WITH n, collect(distinct m) AS nbrs\n",
    "            RETURN n.id AS id,\n",
    "                   n.rdfs_label AS label,\n",
    "                   n.hasTicker AS ticker,\n",
    "                   n.comment AS comment,\n",
    "                   [x IN nbrs | coalesce(x.rdfs_label, x.id)] AS neighbors\n",
    "            ORDER BY label\n",
    "        \"\"\", {\"ids\": ids}).data()\n",
    "    return data\n",
    "\n",
    "def build_llm_context(query: str, top_k: int = 5, depth: int = 2, max_neighbors: int = 8) -> Dict[str, Any]:\n",
    "    hits = vector_search(query, top_k=top_k)\n",
    "    expanded = graph_expand(hits[\"id\"].tolist(), depth=depth)\n",
    "    exp_by_id = {row[\"id\"]: row for row in expanded}\n",
    "    \n",
    "    # Clean URLs to entity names\n",
    "    def clean(s):\n",
    "        if \"FinancialOntology/\" in str(s):\n",
    "            return str(s).split(\"/\")[-1].replace(\"_\", \" \")\n",
    "        return str(s)\n",
    "    \n",
    "    # Build Python dictionary format (Wu & Tsioutsiouliklis, 2024)\n",
    "    relationships = {\"type\": {}, \"issuedBy\": {}, \"isRegulatedBy\": {}, \"hasTicker\": {}}\n",
    "    citations = []\n",
    "    \n",
    "    for i, row in enumerate(hits.to_dict(orient=\"records\"), start=1):\n",
    "        meta = exp_by_id.get(row[\"id\"], {})\n",
    "        label = meta.get(\"label\", row[\"id\"].split(\"/\")[-1])\n",
    "        \n",
    "        citations.append({\"key\": f\"[E{i}]\", \"id\": row[\"id\"], \"label\": label})\n",
    "        \n",
    "        # Add properties\n",
    "        if row.get(\"class_label\"):\n",
    "            relationships[\"type\"][label] = row[\"class_label\"]\n",
    "        if meta.get(\"ticker\"):\n",
    "            relationships[\"hasTicker\"][label] = meta[\"ticker\"]\n",
    "            \n",
    "        # Parse relationships from hybrid text\n",
    "        text = row.get(\"hybridText\", \"\")\n",
    "        if \"issued by\" in text.lower():\n",
    "            issuer = text.split(\"issued by\")[1].split()[0:2]\n",
    "            issuer = \" \".join(issuer).rstrip(\".,\")\n",
    "            relationships[\"issuedBy\"][label] = clean(issuer)\n",
    "        if \"regulated by\" in text.lower():\n",
    "            reg = text.split(\"regulated by\")[1].split()[0]\n",
    "            relationships[\"isRegulatedBy\"][label] = clean(reg)\n",
    "    \n",
    "    # Format context with proper indentation\n",
    "    relationships = {k: v for k, v in relationships.items() if v}\n",
    "    rel_str = chr(10).join(f\"    '{k}': {v},\" for k, v in relationships.items())\n",
    "    \n",
    "    context = f\"\"\"Query: {query}\n",
    "\n",
    "# Financial Knowledge Graph (Python Dictionary Format)\n",
    "relationships = {{\n",
    "{rel_str}\n",
    "}}\n",
    "\n",
    "# Usage: relationships['property']['entity'] returns the value\n",
    "# Example: relationships['type']['AAPL'] returns 'Stock'\"\"\"\n",
    "    \n",
    "    return {\"context\": context, \"hits\": hits, \"citations\": citations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: Example usage (show output)\n",
    "query = \"equities regulated by the SEC\"\n",
    "bundle = build_llm_context(query, top_k=5, depth=2, max_neighbors=8)\n",
    "\n",
    "print(textwrap.dedent(bundle[\"context\"]))\n",
    "print(\"\\nCitations:\")\n",
    "for c in bundle[\"citations\"]:\n",
    "    print(f\"  {c['key']} -> {c['label']} ({c['id']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.3: Return a prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a financial assistant helping users understand investments and securities.\\n\"\n",
    "    \"Your approach to answering questions is to be short, direct, and concises.\\n\"\n",
    "    \"Take a conversational tone in responding to the question,\"\n",
    "    \"but just state the facts and do not spend any extra text expanding on your answer.\\n\"\n",
    "    \"Use the relationships dictionary to answer questions accurately but conversationally.\\n\"\n",
    "    \"Important: Give clear, natural answers without mentioning URLs, technical details, or code.\\n\"\n",
    "    \"When citing sources, use the entity names naturally in your response.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Provide a clear, conversational answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1: Demo queries\n",
    "def make_context(question: str):\n",
    "    bundle = build_llm_context(question, top_k=5, depth=2, max_neighbors=8)\n",
    "    return {\"context\": bundle[\"context\"], \"question\": question}\n",
    "\n",
    "rag_chain = RunnableLambda(make_context) | chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "user_queries = [\n",
    "   \"Is AAPL a stock or bond?\",\n",
    "   \"What type of instrument is USTB?\",\n",
    "   \"Which authority regulates MSFT?\",\n",
    "   \"Which equities are regulated by the SEC, and who issues them?\",\n",
    "   \"What stocks do you know?  What bonds?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(user_queries, start=1):\n",
    "    print(f\"\\n=== Query {i}: {q}\")\n",
    "    try:\n",
    "        ans = rag_chain.invoke(q)\n",
    "        print(ans)\n",
    "    except Exception as e:\n",
    "        print(f\"[error] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
