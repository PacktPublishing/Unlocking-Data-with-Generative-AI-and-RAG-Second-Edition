{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-STEP: Install Required Dependencies\n",
    "%pip install langchain\n",
    "%pip install langgraph\n",
    "%pip install langchain-openai\n",
    "%pip install chromadb\n",
    "%pip install python-dotenv\n",
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and load the trained agent state from Lab 18-1\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the new architecture components\n",
    "from coala_agent import CoALAAgent\n",
    "from domain_investment.investment_advisor_agent import InvestmentAdvisorAgent\n",
    "from domain_investment.investment_advisor_prompts import (\n",
    "    PROMPT_MEMORY_OPTIMIZATION, GRADIENT_CRITIQUE, GRADIENT_PROPOSAL,\n",
    "    METAPROMPT_SURFACE, METAPROMPT_DEEP, METAPROMPT_SYNTHESIS\n",
    ")\n",
    "from domain_investment.investor_test_scenarios import (\n",
    "    run_prompt_memory_test,\n",
    "    run_gradient_test,\n",
    "    test_agents_with_queries,\n",
    "    test_response_consistency,\n",
    "    compare_agent_performance\n",
    ")\n",
    "\n",
    "\n",
    "from domain_agent import DomainProcedure\n",
    "\n",
    "load_dotenv(dotenv_path='env.txt')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61438b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define function to save checkpoints\n",
    "#  Create checkpoint of current state\n",
    "def save_checkpoint(agent: CoALAAgent, checkpoint_name: str):\n",
    "    \"\"\"Save agent state for comparison baseline\"\"\"\n",
    "    # Get procedural memory stats\n",
    "    proc_stats = agent.procedural_memory.get_stats() if hasattr(agent, 'procedural_memory') else {}\n",
    "    \n",
    "    checkpoint = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'episodic_count': len(agent.vector_store.get()[\"ids\"]) if hasattr(agent.vector_store, 'get') else 0,\n",
    "        'procedural_stats': proc_stats,\n",
    "        'learned_strategies': list(agent.procedural_memory.global_procedures.keys()) if hasattr(agent, 'procedural_memory') else [],\n",
    "        'current_performance': proc_stats.get('avg_success_rate', 0)\n",
    "    }\n",
    "    \n",
    "    with open(f\"checkpoints/{checkpoint_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "print(\"🔄 Loading trained agent from Lab 18-1...\")\n",
    "\n",
    "# Create domain agent\n",
    "domain_agent = InvestmentAdvisorAgent()\n",
    "\n",
    "# IMPORTANT: Load the EXISTING agent from Lab 18-1, not create a new one\n",
    "existing_memory_dir = os.path.join(domain_agent.domain_dir, \"domain_memory_store\")\n",
    "\n",
    "if not os.path.exists(existing_memory_dir):\n",
    "    print(\"⚠️  No existing memory store found from Lab 18-1!\")\n",
    "    print(\"   Please run Lab 18-1 first to create the trained agent.\")\n",
    "    raise FileNotFoundError(f\"Memory store not found at {existing_memory_dir}\")\n",
    "\n",
    "# Load the existing trained agent (picking up from Lab 18-1)\n",
    "baseline_agent = CoALAAgent(\n",
    "    domain_agent=domain_agent,\n",
    "    model_name=\"gpt-4.1-mini\",  # Match Lab 18-1\n",
    "    temperature=0,\n",
    "    persist_directory=existing_memory_dir  # Use EXISTING memory from Lab 18-1\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded existing agent from: {existing_memory_dir}\")\n",
    "\n",
    "# Save baseline checkpoint\n",
    "baseline_checkpoint = save_checkpoint(baseline_agent, \"baseline_lab18_2\")\n",
    "\n",
    "print(\"\\n📊 Current State (from Lab 18-1):\")\n",
    "print(f\"  Episodic memories: {baseline_checkpoint['episodic_count']}\")\n",
    "print(f\"  Learned strategies: {len(baseline_checkpoint['learned_strategies'])}\")\n",
    "print(f\"  Current performance: {baseline_checkpoint['current_performance']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf81665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Show procedural memory breakdown by scope\n",
    "# Show breakdown by scope\n",
    "if hasattr(baseline_agent, 'procedural_memory'):\n",
    "    stats = baseline_agent.procedural_memory.get_stats()\n",
    "    print(f\"\\n  Strategy breakdown by scope:\")\n",
    "    for scope, count in stats['by_scope'].items():\n",
    "        print(f\"    {scope.capitalize()}: {count}\")\n",
    "\n",
    "# Load test data for comparison - use conversations NOT seen in Lab 18-1\n",
    "data_dir = os.path.join(domain_agent.domain_dir, \"investment_advisor_data\")\n",
    "test_conversations = []\n",
    "\n",
    "if os.path.exists(f\"{data_dir}/conversations.jsonl\"):\n",
    "    with open(f\"{data_dir}/conversations.jsonl\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # Lab 18-1 used conversations 0-100, so use 100-150 for testing\n",
    "            if i >= 100 and i < 150:  \n",
    "                test_conversations.append(json.loads(line))\n",
    "else:\n",
    "    print(\"⚠️  No conversation data found. Run Lab 18-1 first to generate data.\")\n",
    "\n",
    "if test_conversations:\n",
    "    print(f\"\\n📁 Loaded {len(test_conversations)} NEW test conversations (not seen in Lab 18-1)\")\n",
    "    print(f\"  Success rate in test set: {sum(1 for c in test_conversations if c['feedback']['success']) / len(test_conversations):.1%}\")\n",
    "    print(f\"  Avg satisfaction: {sum(c['feedback']['satisfaction_score'] for c in test_conversations) / len(test_conversations):.1f}/5.0\")\n",
    "\n",
    "# Verify we have the trained model\n",
    "if baseline_checkpoint['episodic_count'] == 0:\n",
    "    print(\"\\n⚠️  WARNING: Agent appears to be untrained (no memories found)\")\n",
    "    print(\"   Please complete Lab 18-1 first to train the agent.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Ready for optimization testing with {len(test_conversations)} new conversations\")\n",
    "    print(\"   We'll test 3 LangMem algorithms on this trained agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517de801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement and test prompt_memory algorithm\n",
    "class PromptMemoryOptimizer:\n",
    "    \"\"\"\n",
    "    Prompt_memory: Single-pass optimization with minimal overhead.\n",
    "    Key characteristics:\n",
    "    - One LLM call for both analysis and synthesis\n",
    "    - Lower computational cost\n",
    "    - Faster adaptation cycles\n",
    "    - Best for simpler patterns and quick iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        # Use the investment-specific optimization prompt\n",
    "        self.optimization_prompt = PromptTemplate.from_template(\n",
    "            PROMPT_MEMORY_OPTIMIZATION\n",
    "        )\n",
    "        self.parser = JsonOutputParser()\n",
    "    \n",
    "    def optimize(self, conversations: List[Dict], current_stats: Dict) -> Dict:\n",
    "        \"\"\"Single-pass optimization\"\"\"\n",
    "        # Format conversations for analysis\n",
    "        formatted_convs = []\n",
    "        for conv in conversations[:10]:  # Limit for context window\n",
    "            formatted_convs.append({\n",
    "                \"query\": conv[\"messages\"][0][\"content\"],\n",
    "                \"response\": conv[\"messages\"][1][\"content\"][:200],\n",
    "                \"success\": conv[\"feedback\"][\"success\"],\n",
    "                \"satisfaction\": conv[\"feedback\"][\"satisfaction_score\"]\n",
    "            })\n",
    "        \n",
    "        # Single LLM call for optimization\n",
    "        chain = self.optimization_prompt | self.llm | self.parser\n",
    "        result = chain.invoke({\n",
    "            \"conversations\": json.dumps(formatted_convs, indent=2),\n",
    "            \"current_performance\": json.dumps(current_stats)\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"algorithm\": \"prompt_memory\",\n",
    "            \"patterns\": result.get(\"patterns_found\", []),\n",
    "            \"rules\": result.get(\"procedural_rules\", []),\n",
    "            \"summary\": result.get(\"optimization_summary\", \"\"),\n",
    "            \"llm_calls\": 1  # Key efficiency metric\n",
    "        }\n",
    "\n",
    "# TESTING:\n",
    "# Create new domain agent instance for testing\n",
    "test_domain_agent = InvestmentAdvisorAgent()\n",
    "\n",
    "# Initialize optimizer\n",
    "prompt_optimizer = PromptMemoryOptimizer(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    ")\n",
    "\n",
    "# Run the complete test\n",
    "test_results = run_prompt_memory_test(\n",
    "    baseline_agent, \n",
    "    test_domain_agent, \n",
    "    prompt_optimizer, \n",
    "    test_conversations\n",
    ")\n",
    "\n",
    "if test_results:\n",
    "    # Display results\n",
    "    print(f\"\\n📊 Prompt_memory Results:\")\n",
    "    print(f\"  Time taken: {test_results['optimization_time']:.2f} seconds\")\n",
    "    print(f\"  LLM calls: {test_results['prompt_result']['llm_calls']}\")\n",
    "    print(f\"  Patterns found: {len(test_results['prompt_result']['patterns'])}\") \n",
    "    print(f\"  Rules generated: {len(test_results['prompt_result']['rules'])}\")\n",
    "    \n",
    "    if test_results['prompt_result']['rules']:\n",
    "        print(f\"\\n  Sample rule: {test_results['prompt_result']['rules'][0]['rule']}\")\n",
    "        print(f\"  Priority: {test_results['prompt_result']['rules'][0].get('priority', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n✅ Applied {test_results['applied_rules']} rules to agent\")\n",
    "    \n",
    "    # Show efficiency results\n",
    "    print(\"\\n⏱️ Testing efficiency (3 runs)...\")\n",
    "    eff = test_results['efficiency']\n",
    "    print(f\"  Average optimization time: {eff['avg_time']:.2f} seconds\")\n",
    "    print(f\"  Efficiency: {eff['efficiency']:.1f} optimizations/second\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    prompt_checkpoint = save_checkpoint(test_results['agent'], \"prompt_memory_test\")\n",
    "    print(f\"\\n💾 Checkpoint saved: {len(prompt_checkpoint['learned_strategies'])} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Implement and test gradient algorithm\n",
    "class GradientOptimizer:\n",
    "    \"\"\"\n",
    "    Gradient: Two-phase optimization with separated critique and proposal.\n",
    "    Key characteristics:\n",
    "    - Phase 1: Objective critique of current behavior\n",
    "    - Phase 2: Targeted improvement proposals\n",
    "    - More focused and actionable rules\n",
    "    - Better for identifying specific failure modes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Use investment-specific gradient prompts\n",
    "        self.critique_prompt = PromptTemplate.from_template(GRADIENT_CRITIQUE)\n",
    "        self.proposal_prompt = PromptTemplate.from_template(GRADIENT_PROPOSAL)\n",
    "        \n",
    "        self.parser = JsonOutputParser()\n",
    "    \n",
    "    def optimize(self, conversations: List[Dict], current_stats: Dict) -> Dict:\n",
    "        \"\"\"Two-phase gradient optimization\"\"\"\n",
    "        \n",
    "        # Format conversations\n",
    "        formatted_convs = []\n",
    "        for conv in conversations[:10]:\n",
    "            formatted_convs.append({\n",
    "                \"query\": conv[\"messages\"][0][\"content\"],\n",
    "                \"response\": conv[\"messages\"][1][\"content\"][:200],\n",
    "                \"success\": conv[\"feedback\"][\"success\"],\n",
    "                \"satisfaction\": conv[\"feedback\"][\"satisfaction_score\"],\n",
    "                \"signals\": list(k for k, v in conv[\"behavioral_signals\"].items() if v)\n",
    "            })\n",
    "        \n",
    "        # Phase 1: Critique\n",
    "        critique_chain = self.critique_prompt | self.llm | self.parser\n",
    "        critique = critique_chain.invoke({\n",
    "            \"conversations\": json.dumps(formatted_convs, indent=2)\n",
    "        })\n",
    "        \n",
    "        # Phase 2: Proposal based on critique\n",
    "        proposal_chain = self.proposal_prompt | self.llm | self.parser\n",
    "        proposal = proposal_chain.invoke({\n",
    "            \"critique\": json.dumps(critique, indent=2)\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"algorithm\": \"gradient\",\n",
    "            \"critique\": critique,\n",
    "            \"improvements\": proposal.get(\"improvements\", []),\n",
    "            \"strategy\": proposal.get(\"optimization_strategy\", \"\"),\n",
    "            \"llm_calls\": 2  # Two-phase approach\n",
    "        }\n",
    "\n",
    "# TESTING: \n",
    "# Create new domain agent instance for testing\n",
    "gradient_domain_agent = InvestmentAdvisorAgent()\n",
    "\n",
    "# Initialize gradient optimizer\n",
    "gradient_optimizer = GradientOptimizer(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    ")\n",
    "\n",
    "# Run the complete test\n",
    "test_results = run_gradient_test(\n",
    "    baseline_agent,\n",
    "    gradient_domain_agent,\n",
    "    gradient_optimizer,\n",
    "    test_conversations\n",
    ")\n",
    "\n",
    "if test_results:\n",
    "    result = test_results['gradient_result']\n",
    "    \n",
    "    # Display optimization results\n",
    "    print(f\"\\n📊 Gradient Results:\")\n",
    "    print(f\"  Time taken: {test_results['optimization_time']:.2f} seconds\")\n",
    "    print(f\"  LLM calls: {result['llm_calls']}\")\n",
    "    print(f\"  Issues identified: {len(result['critique'].get('failures', []))}\")\n",
    "    print(f\"  Successes noted: {len(result['critique'].get('successes', []))}\")\n",
    "    print(f\"  Critical gaps: {len(result['critique'].get('critical_gaps', []))}\")\n",
    "    print(f\"  Improvements generated: {len(result['improvements'])}\")\n",
    "    \n",
    "    if result['improvements']:\n",
    "        improvement = result['improvements'][0]\n",
    "        print(f\"\\n  Sample improvement:\")\n",
    "        print(f\"    Target: {improvement.get('target_issue', 'N/A')}\")\n",
    "        print(f\"    Rule: {improvement.get('rule', 'N/A')}\")\n",
    "        print(f\"    Impact: {improvement.get('expected_impact', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n✅ Applied {test_results['applied_improvements']} improvements to agent\")\n",
    "    \n",
    "    # Display issue detection results\n",
    "    print(\"\\n🔍 Testing issue detection capability...\")\n",
    "    issue_detection = test_results['issue_detection']\n",
    "    if issue_detection.get('critical_gaps'):\n",
    "        print(f\"  Identified gaps: {len(issue_detection['critical_gaps'])} issues\")\n",
    "        for gap in issue_detection['critical_gaps'][:1]:\n",
    "            print(f\"    • {gap.get('gap', 'Issue detected')}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    gradient_checkpoint = save_checkpoint(test_results['agent'], \"gradient_test\")\n",
    "    print(f\"\\n💾 Checkpoint saved: {len(gradient_checkpoint['learned_strategies'])} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Implement and test metaprompt algorithm\n",
    "class MetapromptOptimizer:\n",
    "    \"\"\"\n",
    "    Metaprompt: Multi-stage analysis with reflection for deep pattern discovery.\n",
    "    Key characteristics:\n",
    "    - Multiple analysis stages with different perspectives\n",
    "    - Reflection steps to question assumptions\n",
    "    - Discovers non-obvious patterns and correlations\n",
    "    - Best for complex domains requiring nuanced understanding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Use investment-specific metaprompt prompts\n",
    "        self.surface_prompt = PromptTemplate.from_template(METAPROMPT_SURFACE)\n",
    "        self.deep_prompt = PromptTemplate.from_template(METAPROMPT_DEEP)\n",
    "        self.synthesis_prompt = PromptTemplate.from_template(METAPROMPT_SYNTHESIS)\n",
    "        \n",
    "        self.parser = JsonOutputParser()\n",
    "    \n",
    "    def optimize(self, conversations: List[Dict], current_stats: Dict) -> Dict:\n",
    "        \"\"\"Multi-stage metaprompt optimization with reflection\"\"\"\n",
    "        \n",
    "        # Format conversations with rich context\n",
    "        formatted_convs = []\n",
    "        for conv in conversations[:10]:\n",
    "            formatted_convs.append({\n",
    "                \"query\": conv[\"messages\"][0][\"content\"],\n",
    "                \"response\": conv[\"messages\"][1][\"content\"][:200],\n",
    "                \"success\": conv[\"feedback\"][\"success\"],\n",
    "                \"satisfaction\": conv[\"feedback\"][\"satisfaction_score\"],\n",
    "                \"behavioral_signals\": conv[\"behavioral_signals\"],\n",
    "                \"metadata\": conv[\"metadata\"]\n",
    "            })\n",
    "        \n",
    "        # Stage 1: Surface analysis\n",
    "        surface_chain = self.surface_prompt | self.llm | self.parser\n",
    "        surface_analysis = surface_chain.invoke({\n",
    "            \"conversations\": json.dumps(formatted_convs, indent=2)\n",
    "        })\n",
    "        \n",
    "        # Stage 2: Deep analysis with reflection\n",
    "        deep_chain = self.deep_prompt | self.llm | self.parser\n",
    "        deep_analysis = deep_chain.invoke({\n",
    "            \"surface_analysis\": json.dumps(surface_analysis, indent=2),\n",
    "            \"conversations\": json.dumps(formatted_convs, indent=2)\n",
    "        })\n",
    "        \n",
    "        # Stage 3: Synthesis\n",
    "        synthesis_chain = self.synthesis_prompt | self.llm | self.parser\n",
    "        synthesis = synthesis_chain.invoke({\n",
    "            \"surface_analysis\": json.dumps(surface_analysis, indent=2),\n",
    "            \"deep_analysis\": json.dumps(deep_analysis, indent=2)\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"algorithm\": \"metaprompt\",\n",
    "            \"surface_patterns\": surface_analysis.get(\"surface_patterns\", []),\n",
    "            \"hidden_patterns\": deep_analysis.get(\"hidden_patterns\", []),\n",
    "            \"causal_relationships\": deep_analysis.get(\"causal_relationships\", []),\n",
    "            \"comprehensive_rules\": synthesis.get(\"comprehensive_rules\", []),\n",
    "            \"meta_insights\": synthesis.get(\"meta_insights\", \"\"),\n",
    "            \"llm_calls\": 3  # Three-stage approach\n",
    "        }\n",
    "    \n",
    "# TESTING:\n",
    "\n",
    "from domain_investment.investor_test_scenarios import run_metaprompt_test\n",
    "\n",
    "# Create new domain agent instance for testing\n",
    "metaprompt_domain_agent = InvestmentAdvisorAgent()\n",
    "\n",
    "# Initialize metaprompt optimizer\n",
    "metaprompt_optimizer = MetapromptOptimizer(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    ")\n",
    "\n",
    "# Run the complete test\n",
    "test_results = run_metaprompt_test(\n",
    "    baseline_agent,\n",
    "    metaprompt_domain_agent,\n",
    "    metaprompt_optimizer,\n",
    "    test_conversations\n",
    ")\n",
    "\n",
    "if test_results:\n",
    "    result = test_results['metaprompt_result']\n",
    "    \n",
    "    # Display optimization results\n",
    "    print(f\"\\n📊 Metaprompt Results:\")\n",
    "    print(f\"  Time taken: {test_results['optimization_time']:.2f} seconds\")\n",
    "    print(f\"  LLM calls: {result['llm_calls']}\")\n",
    "    print(f\"  Surface patterns: {len(result['surface_patterns'])}\")\n",
    "    print(f\"  Hidden patterns: {len(result['hidden_patterns'])}\")\n",
    "    print(f\"  Causal relationships: {len(result['causal_relationships'])}\")\n",
    "    print(f\"  Comprehensive rules: {len(result['comprehensive_rules'])}\")\n",
    "    \n",
    "    if result['hidden_patterns']:\n",
    "        hidden = result['hidden_patterns'][0]\n",
    "        print(f\"\\n  Hidden pattern discovered:\")\n",
    "        print(f\"    Pattern: {hidden.get('pattern', 'N/A')}\")\n",
    "        print(f\"    Confidence: {hidden.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    if result['meta_insights']:\n",
    "        print(f\"\\n  Meta insight: {result['meta_insights'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n✅ Applied {test_results['applied_rules']} comprehensive rules to agent\")\n",
    "    \n",
    "    # Display reflection test results\n",
    "    print(\"\\n🔍 Testing reflection capability...\")\n",
    "    reflection = test_results['reflection_test']\n",
    "    \n",
    "    if reflection['causal_relationships']:\n",
    "        print(f\"  Discovered causal relationship:\")\n",
    "        causal = reflection['causal_relationships'][0]\n",
    "        print(f\"    Cause: {causal.get('cause', 'N/A')}\")\n",
    "        print(f\"    Effect: {causal.get('effect', 'N/A')}\")\n",
    "        print(f\"    Mechanism: {causal.get('mechanism', 'N/A')}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    metaprompt_checkpoint = save_checkpoint(test_results['agent'], \"metaprompt_test\")\n",
    "    print(f\"\\n💾 Checkpoint saved: {len(metaprompt_checkpoint['learned_strategies'])} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7738da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run comprehensive comparison of all three algorithms\n",
    "class AlgorithmComparison:\n",
    "    \"\"\"Compare optimization algorithms across multiple dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def compare_algorithms(self, \n",
    "                          test_convs: List[Dict],\n",
    "                          optimizers: Dict) -> Dict:\n",
    "        \"\"\"Run all optimizers and compare results\"\"\"\n",
    "        \n",
    "        comparison = {\n",
    "            \"timing\": {},\n",
    "            \"quality\": {},\n",
    "            \"efficiency\": {}\n",
    "        }\n",
    "        \n",
    "        for name, optimizer in optimizers.items():\n",
    "            print(f\"\\n🔄 Testing {name}...\")\n",
    "            \n",
    "            # Measure optimization\n",
    "            start_time = datetime.now()\n",
    "            result = optimizer.optimize(test_convs, {\"avg_success_rate\": 0.7})\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            # Timing metrics\n",
    "            comparison[\"timing\"][name] = {\n",
    "                \"total_seconds\": (end_time - start_time).total_seconds(),\n",
    "                \"llm_calls\": result[\"llm_calls\"],\n",
    "                \"seconds_per_call\": (end_time - start_time).total_seconds() / result[\"llm_calls\"]\n",
    "            }\n",
    "            \n",
    "            # Quality metrics\n",
    "            if name == \"prompt_memory\":\n",
    "                rule_count = len(result.get(\"rules\", []))\n",
    "                pattern_count = len(result.get(\"patterns\", []))\n",
    "            elif name == \"gradient\":\n",
    "                rule_count = len(result.get(\"improvements\", []))\n",
    "                pattern_count = len(result.get(\"critique\", {}).get(\"successes\", []))\n",
    "            else:  # metaprompt\n",
    "                rule_count = len(result.get(\"comprehensive_rules\", []))\n",
    "                pattern_count = len(result.get(\"hidden_patterns\", [])) + len(result.get(\"surface_patterns\", []))\n",
    "            \n",
    "            comparison[\"quality\"][name] = {\n",
    "                \"rules_generated\": rule_count,\n",
    "                \"patterns_found\": pattern_count,\n",
    "                \"has_examples\": name == \"metaprompt\",\n",
    "                \"has_causal_analysis\": name in [\"gradient\", \"metaprompt\"]\n",
    "            }\n",
    "            \n",
    "            # Efficiency score\n",
    "            comparison[\"efficiency\"][name] = {\n",
    "                \"patterns_per_second\": pattern_count / comparison[\"timing\"][name][\"total_seconds\"] if comparison[\"timing\"][name][\"total_seconds\"] > 0 else 0,\n",
    "                \"rules_per_second\": rule_count / comparison[\"timing\"][name][\"total_seconds\"] if comparison[\"timing\"][name][\"total_seconds\"] > 0 else 0,\n",
    "                \"cost_estimate\": result[\"llm_calls\"] * 0.002  # Rough cost estimate\n",
    "            }\n",
    "            \n",
    "            self.results[name] = result\n",
    "        \n",
    "        return comparison\n",
    "\n",
    "# TESTING:\n",
    "\n",
    "# Verify procedural memory state\n",
    "print(\"📊 Current procedural memory state:\")\n",
    "print(f\"  Baseline procedures: {len(baseline_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Prompt_memory procedures: {len(prompt_memory_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Gradient procedures: {len(gradient_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Metaprompt procedures: {len(metaprompt_agent.procedural_memory.global_procedures)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🧪 TESTING OPTIMIZED AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test all agents\n",
    "agents_to_test = [\n",
    "    (\"baseline\", baseline_agent),\n",
    "    (\"prompt_memory\", prompt_memory_agent),\n",
    "    (\"gradient\", gradient_agent),\n",
    "    (\"metaprompt\", metaprompt_agent)\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results_by_algo = test_agents_with_queries(agents_to_test)\n",
    "\n",
    "# Generate comparison statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RESPONSE QUALITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = compare_agent_performance(results_by_algo)\n",
    "\n",
    "for algo_name, algo_stats in stats.items():\n",
    "    improvement = \" (+improved)\" if algo_stats.get(\"empathy_improvement\", False) else \"\"\n",
    "    \n",
    "    print(f\"\\n{algo_name.upper()}:\")\n",
    "    print(f\"  Empathy shown: {algo_stats['empathy_count']}/{algo_stats['total_queries']} responses{improvement}\")\n",
    "    print(f\"  Specific data: {algo_stats['specifics_count']}/{algo_stats['total_queries']} responses\")\n",
    "    print(f\"  Educational: {algo_stats['education_count']}/{algo_stats['total_queries']} responses\")\n",
    "    print(f\"  Avg length: {algo_stats['avg_length']:.0f} characters\")\n",
    "\n",
    "# Test consistency\n",
    "print(\"\\n📈 CONSISTENCY TEST:\")\n",
    "consistency_results = test_response_consistency(agents_to_test)\n",
    "\n",
    "print(\"Response consistency (3 runs):\")\n",
    "for algo, stats in consistency_results.items():\n",
    "    print(f\"  {algo}: mean={stats['mean_length']:.0f} chars, std={stats['std_length']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe046d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test each optimized agent with real scenarios\n",
    "def apply_optimization_to_agent(agent: CoALAAgent, \n",
    "                               optimization_result: Dict) -> None:\n",
    "    \"\"\"Apply optimization results to agent's procedural memory\"\"\"\n",
    "    \n",
    "    if optimization_result[\"algorithm\"] == \"prompt_memory\":\n",
    "        # Apply prompt_memory rules\n",
    "        for rule in optimization_result.get(\"rules\", []):\n",
    "            procedure = DomainProcedure(\n",
    "                strategy_pattern=rule.get(\"rule\", \"Rule\"),\n",
    "                steps=[rule.get(\"rule\", \"Apply rule\")],\n",
    "                success_rate=0.8,\n",
    "                segments=[\"general\"],\n",
    "                scope=\"global\"\n",
    "            )\n",
    "            key = rule.get(\"rule\", \"rule\")[:30] if rule.get(\"rule\") else f\"rule_{len(agent.procedural_memory.global_procedures)}\"\n",
    "            agent.procedural_memory.global_procedures[key] = procedure\n",
    "    \n",
    "    elif optimization_result[\"algorithm\"] == \"gradient\":\n",
    "        # Apply gradient improvements\n",
    "        for improvement in optimization_result.get(\"improvements\", []):\n",
    "            procedure = DomainProcedure(\n",
    "                strategy_pattern=improvement.get(\"rule\", \"Improvement\"),\n",
    "                steps=[improvement.get(\"rule\", \"Apply improvement\"), \n",
    "                       improvement.get(\"implementation\", \"Apply when appropriate\")],\n",
    "                success_rate=0.85,\n",
    "                segments=[\"general\"],\n",
    "                scope=\"global\"\n",
    "            )\n",
    "            key = improvement.get(\"target_issue\", \"issue\")[:30] if improvement.get(\"target_issue\") else f\"improvement_{len(agent.procedural_memory.global_procedures)}\"\n",
    "            agent.procedural_memory.global_procedures[key] = procedure\n",
    "    \n",
    "    else:  # metaprompt\n",
    "        # Apply comprehensive rules with examples\n",
    "        for rule in optimization_result.get(\"comprehensive_rules\", []):\n",
    "            procedure = DomainProcedure(\n",
    "                strategy_pattern=rule.get(\"rule\", \"Comprehensive rule\"),\n",
    "                steps=[rule.get(\"rule\", \"Apply rule\"), \n",
    "                       f\"Example: {rule.get('example_application', 'N/A')}\"],\n",
    "                success_rate=rule.get(\"confidence\", 0.9),\n",
    "                segments=[\"general\"],\n",
    "                scope=\"global\"\n",
    "            )\n",
    "            key = rule.get(\"rule\", \"rule\")[:30] if rule.get(\"rule\") else f\"metarule_{len(agent.procedural_memory.global_procedures)}\"\n",
    "            agent.procedural_memory.global_procedures[key] = procedure\n",
    "\n",
    "# TESTING:\n",
    "\n",
    "# Verify procedural memory state\n",
    "print(\"📊 Current procedural memory state:\")\n",
    "print(f\"  Baseline procedures: {len(baseline_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Prompt_memory procedures: {len(prompt_memory_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Gradient procedures: {len(gradient_agent.procedural_memory.global_procedures)}\")\n",
    "print(f\"  Metaprompt procedures: {len(metaprompt_agent.procedural_memory.global_procedures)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🧪 TESTING OPTIMIZED AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test all agents\n",
    "agents_to_test = [\n",
    "    (\"baseline\", baseline_agent),\n",
    "    (\"prompt_memory\", prompt_memory_agent),\n",
    "    (\"gradient\", gradient_agent),\n",
    "    (\"metaprompt\", metaprompt_agent)\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "results_by_algo = test_agents_with_queries(agents_to_test)\n",
    "\n",
    "# Generate comparison statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RESPONSE QUALITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = compare_agent_performance(results_by_algo)\n",
    "\n",
    "for algo_name, algo_stats in stats.items():\n",
    "    improvement = \" (+improved)\" if algo_stats.get(\"empathy_improvement\", False) else \"\"\n",
    "    \n",
    "    print(f\"\\n{algo_name.upper()}:\")\n",
    "    print(f\"  Empathy shown: {algo_stats['empathy_count']}/{algo_stats['total_queries']} responses{improvement}\")\n",
    "    print(f\"  Specific data: {algo_stats['specifics_count']}/{algo_stats['total_queries']} responses\")\n",
    "    print(f\"  Educational: {algo_stats['education_count']}/{algo_stats['total_queries']} responses\")\n",
    "    print(f\"  Avg length: {algo_stats['avg_length']:.0f} characters\")\n",
    "\n",
    "# Test consistency\n",
    "print(\"\\n📈 CONSISTENCY TEST:\")\n",
    "consistency_results = test_response_consistency(agents_to_test)\n",
    "\n",
    "print(\"Response consistency (3 runs):\")\n",
    "for algo, stats in consistency_results.items():\n",
    "    print(f\"  {algo}: mean={stats['mean_length']:.0f} chars, std={stats['std_length']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS Cell 9: Create decision framework for algorithm selection\n",
    "class AlgorithmSelector:\n",
    "    \"\"\"Decision framework for selecting optimal LangMem algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, comparison_results: Dict):\n",
    "        self.comparison = comparison_results\n",
    "        self.recommendations = []\n",
    "    \n",
    "    def recommend_algorithm(self, \n",
    "                          requirements: Dict) -> Dict:\n",
    "        \"\"\"Recommend best algorithm based on requirements\"\"\"\n",
    "        \n",
    "        scores = {\n",
    "            \"prompt_memory\": 0,\n",
    "            \"gradient\": 0,\n",
    "            \"metaprompt\": 0\n",
    "        }\n",
    "        \n",
    "        reasoning = {\n",
    "            \"prompt_memory\": [],\n",
    "            \"gradient\": [],\n",
    "            \"metaprompt\": []\n",
    "        }\n",
    "        \n",
    "        # Speed requirement\n",
    "        if requirements.get(\"max_latency_seconds\", float('inf')) < 3:\n",
    "            scores[\"prompt_memory\"] += 3\n",
    "            reasoning[\"prompt_memory\"].append(\"Meets strict latency requirement\")\n",
    "        elif requirements.get(\"max_latency_seconds\", float('inf')) < 5:\n",
    "            scores[\"prompt_memory\"] += 2\n",
    "            scores[\"gradient\"] += 2\n",
    "            reasoning[\"gradient\"].append(\"Acceptable latency\")\n",
    "        \n",
    "        # Cost sensitivity\n",
    "        if requirements.get(\"minimize_cost\", False):\n",
    "            scores[\"prompt_memory\"] += 3\n",
    "            reasoning[\"prompt_memory\"].append(\"Lowest cost per optimization\")\n",
    "        \n",
    "        # Quality requirements\n",
    "        if requirements.get(\"needs_causal_analysis\", False):\n",
    "            scores[\"gradient\"] += 2\n",
    "            scores[\"metaprompt\"] += 3\n",
    "            reasoning[\"metaprompt\"].append(\"Provides deep causal analysis\")\n",
    "        \n",
    "        if requirements.get(\"needs_examples\", False):\n",
    "            scores[\"metaprompt\"] += 3\n",
    "            reasoning[\"metaprompt\"].append(\"Generates detailed examples\")\n",
    "        \n",
    "        # Domain complexity\n",
    "        complexity = requirements.get(\"domain_complexity\", \"medium\")\n",
    "        if complexity == \"simple\":\n",
    "            scores[\"prompt_memory\"] += 2\n",
    "            reasoning[\"prompt_memory\"].append(\"Sufficient for simple domains\")\n",
    "        elif complexity == \"complex\":\n",
    "            scores[\"metaprompt\"] += 3\n",
    "            reasoning[\"metaprompt\"].append(\"Best for complex pattern discovery\")\n",
    "        else:  # medium\n",
    "            scores[\"gradient\"] += 2\n",
    "            reasoning[\"gradient\"].append(\"Balanced for medium complexity\")\n",
    "        \n",
    "        # Real-time learning\n",
    "        if requirements.get(\"real_time_learning\", False):\n",
    "            scores[\"prompt_memory\"] += 3\n",
    "            reasoning[\"prompt_memory\"].append(\"Enables real-time adaptation\")\n",
    "        \n",
    "        # Find best match\n",
    "        best_algo = max(scores, key=scores.get)\n",
    "        \n",
    "        return {\n",
    "            \"recommended\": best_algo,\n",
    "            \"scores\": scores,\n",
    "            \"reasoning\": reasoning[best_algo],\n",
    "            \"alternative\": sorted(scores, key=scores.get, reverse=True)[1]\n",
    "        }\n",
    "\n",
    "# Only run if we have comparison results\n",
    "if 'comparison_results' in locals():\n",
    "    # Initialize selector\n",
    "    selector = AlgorithmSelector(comparison_results)\n",
    "    \n",
    "    # Define different use cases\n",
    "    use_cases = [\n",
    "        {\n",
    "            \"name\": \"High-Frequency Trading Advisor\",\n",
    "            \"requirements\": {\n",
    "                \"max_latency_seconds\": 2,\n",
    "                \"minimize_cost\": True,\n",
    "                \"real_time_learning\": True,\n",
    "                \"domain_complexity\": \"simple\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Retirement Planning Specialist\",\n",
    "            \"requirements\": {\n",
    "                \"max_latency_seconds\": 10,\n",
    "                \"needs_causal_analysis\": True,\n",
    "                \"needs_examples\": True,\n",
    "                \"domain_complexity\": \"complex\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"General Investment Assistant\",\n",
    "            \"requirements\": {\n",
    "                \"max_latency_seconds\": 5,\n",
    "                \"minimize_cost\": False,\n",
    "                \"needs_causal_analysis\": True,\n",
    "                \"domain_complexity\": \"medium\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 ALGORITHM RECOMMENDATIONS BY USE CASE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate recommendations for each use case\n",
    "    recommendations_summary = []\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        recommendation = selector.recommend_algorithm(use_case[\"requirements\"])\n",
    "        \n",
    "        print(f\"\\n📋 {use_case['name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Recommended: {recommendation['recommended'].upper()}\")\n",
    "        print(f\"  Alternative: {recommendation['alternative']}\")\n",
    "        print(f\"  Reasoning:\")\n",
    "        for reason in recommendation['reasoning']:\n",
    "            print(f\"    • {reason}\")\n",
    "        \n",
    "        print(f\"\\n  Algorithm scores:\")\n",
    "        for algo, score in sorted(recommendation['scores'].items(), key=lambda x: x[1], reverse=True):\n",
    "            bar = \"█\" * score + \"░\" * (10 - score)\n",
    "            print(f\"    {algo:15} {bar} ({score})\")\n",
    "        \n",
    "        recommendations_summary.append({\n",
    "            \"use_case\": use_case[\"name\"],\n",
    "            \"recommended\": recommendation['recommended'],\n",
    "            \"score\": recommendation['scores'][recommendation['recommended']]\n",
    "        })\n",
    "    \n",
    "    # Create final summary matrix\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 FINAL RECOMMENDATION MATRIX\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    matrix_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Algorithm\": \"prompt_memory\",\n",
    "            \"Best For\": \"Real-time, High-frequency, Cost-sensitive\",\n",
    "            \"Speed\": \"⚡ Fast\",\n",
    "            \"Quality\": \"★★★☆☆\",\n",
    "            \"Cost\": \"$\",\n",
    "            \"When to Use\": \"Production systems with <3s latency requirements\"\n",
    "        },\n",
    "        {\n",
    "            \"Algorithm\": \"gradient\",\n",
    "            \"Best For\": \"Balanced optimization, Targeted improvements\",\n",
    "            \"Speed\": \"⚡⚡ Medium\",\n",
    "            \"Quality\": \"★★★★☆\",\n",
    "            \"Cost\": \"$$\",\n",
    "            \"When to Use\": \"Most general-purpose applications\"\n",
    "        },\n",
    "        {\n",
    "            \"Algorithm\": \"metaprompt\",\n",
    "            \"Best For\": \"Complex domains, Deep insights, Research\",\n",
    "            \"Speed\": \"⚡⚡⚡ Slow\",\n",
    "            \"Quality\": \"★★★★★\",\n",
    "            \"Cost\": \"$$$\",\n",
    "            \"When to Use\": \"Complex advisory, Offline optimization, Quality critical\"\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\" + matrix_df.to_string(index=False))\n",
    "    \n",
    "    # Performance-based recommendation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"💡 PERFORMANCE-BASED RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'timing_df' in locals() and 'quality_df' in locals():\n",
    "        # Find fastest algorithm\n",
    "        fastest = timing_df['total_seconds'].idxmin()\n",
    "        print(f\"\\n  ⚡ Fastest: {fastest}\")\n",
    "        print(f\"     Average time: {timing_df.loc[fastest, 'total_seconds']:.2f}s\")\n",
    "        \n",
    "        # Find highest quality\n",
    "        if not quality_df.empty:\n",
    "            quality_scores = quality_df['rules_generated'] + quality_df['patterns_found']\n",
    "            best_quality = quality_scores.idxmax()\n",
    "            print(f\"\\n  ⭐ Highest Quality: {best_quality}\")\n",
    "            print(f\"     Rules + Patterns: {quality_scores[best_quality]:.0f}\")\n",
    "        \n",
    "        # Find best value (quality/time ratio)\n",
    "        if 'efficiency_df' in locals() and not efficiency_df.empty:\n",
    "            best_efficiency = efficiency_df['patterns_per_second'].idxmax()\n",
    "            print(f\"\\n  💰 Best Value: {best_efficiency}\")\n",
    "            print(f\"     Patterns/second: {efficiency_df.loc[best_efficiency, 'patterns_per_second']:.2f}\")\n",
    "    \n",
    "    print(\"\\n✅ Decision framework complete\")\n",
    "else:\n",
    "    print(\"⚠️ No comparison results available. Run Cell 5 first to generate comparison data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS Cell 10: Implement dynamic algorithm switching\n",
    "class DynamicOptimizer:\n",
    "    \"\"\"\n",
    "    Dynamically select optimization algorithm based on context.\n",
    "    Demonstrates production pattern for adaptive learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimizers = {\n",
    "            \"prompt_memory\": prompt_optimizer,\n",
    "            \"gradient\": gradient_optimizer,\n",
    "            \"metaprompt\": metaprompt_optimizer\n",
    "        }\n",
    "        self.usage_stats = {algo: {\"calls\": 0, \"total_time\": 0} for algo in self.optimizers}\n",
    "    \n",
    "    def select_optimizer(self, context: Dict) -> tuple:\n",
    "        \"\"\"Select best optimizer for current context\"\"\"\n",
    "        \n",
    "        # Decision logic based on context\n",
    "        if context.get(\"user_waiting\", False):\n",
    "            # User is waiting for response - use fastest\n",
    "            return \"prompt_memory\", self.optimizers[\"prompt_memory\"]\n",
    "        \n",
    "        elif context.get(\"optimization_type\") == \"scheduled\":\n",
    "            # Scheduled batch optimization - use best quality\n",
    "            return \"metaprompt\", self.optimizers[\"metaprompt\"]\n",
    "        \n",
    "        elif context.get(\"failure_rate\", 0) > 0.3:\n",
    "            # High failure rate - need targeted improvements\n",
    "            return \"gradient\", self.optimizers[\"gradient\"]\n",
    "        \n",
    "        else:\n",
    "            # Default to balanced approach\n",
    "            return \"gradient\", self.optimizers[\"gradient\"]\n",
    "    \n",
    "    def optimize_with_context(self, \n",
    "                             conversations: List[Dict],\n",
    "                             context: Dict) -> Dict:\n",
    "        \"\"\"Optimize using context-appropriate algorithm\"\"\"\n",
    "        \n",
    "        algo_name, optimizer = self.select_optimizer(context)\n",
    "        \n",
    "        print(f\"Selected {algo_name} based on context:\")\n",
    "        for key, value in context.items():\n",
    "            print(f\"   - {key}: {value}\")\n",
    "        \n",
    "        # Run optimization\n",
    "        start_time = datetime.now()\n",
    "        result = optimizer.optimize(conversations, {\"avg_success_rate\": 0.7})\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Track usage\n",
    "        self.usage_stats[algo_name][\"calls\"] += 1\n",
    "        self.usage_stats[algo_name][\"total_time\"] += elapsed\n",
    "        \n",
    "        result[\"selected_algorithm\"] = algo_name\n",
    "        result[\"selection_reason\"] = context\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Only run if optimizers exist\n",
    "if test_conversations and 'prompt_optimizer' in locals() and 'gradient_optimizer' in locals() and 'metaprompt_optimizer' in locals():\n",
    "    # Test dynamic optimization\n",
    "    dynamic_opt = DynamicOptimizer()\n",
    "    \n",
    "    # Different contexts requiring different algorithms\n",
    "    contexts = [\n",
    "        {\n",
    "            \"scenario\": \"User asking question in chat\",\n",
    "            \"context\": {\"user_waiting\": True, \"response_needed\": \"immediate\"}\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"Nightly batch optimization\",\n",
    "            \"context\": {\"optimization_type\": \"scheduled\", \"time_constraint\": \"none\"}\n",
    "        },\n",
    "        {\n",
    "            \"scenario\": \"High failure rate detected\",\n",
    "            \"context\": {\"failure_rate\": 0.4, \"urgent\": True}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DYNAMIC ALGORITHM SELECTION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for ctx in contexts:\n",
    "        print(f\"\\nScenario: {ctx['scenario']}\")\n",
    "        result = dynamic_opt.optimize_with_context(\n",
    "            test_conversations[:5],\n",
    "            ctx['context']\n",
    "        )\n",
    "        print(f\"Optimization complete using {result['selected_algorithm']}\")\n",
    "        print(f\"   LLM calls: {result['llm_calls']}\")\n",
    "        if result.get('rules'):\n",
    "            print(f\"   Rules generated: {len(result['rules'])}\")\n",
    "        elif result.get('improvements'):\n",
    "            print(f\"   Improvements: {len(result['improvements'])}\")\n",
    "        elif result.get('comprehensive_rules'):\n",
    "            print(f\"   Comprehensive rules: {len(result['comprehensive_rules'])}\")\n",
    "    \n",
    "    # Display usage statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALGORITHM USAGE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    for algo, stats in dynamic_opt.usage_stats.items():\n",
    "        if stats[\"calls\"] > 0:\n",
    "            avg_time = stats[\"total_time\"] / stats[\"calls\"]\n",
    "            print(f\"  {algo:15} - Calls: {stats['calls']}, Avg Time: {avg_time:.2f}s\")\n",
    "else:\n",
    "    print(\"Prerequisites not available. Run cells 2-4 first to create optimizers.\")\n",
    "\n",
    "# Final summary and best practices\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY TAKEAWAYS AND BEST PRACTICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_practices = \"\"\"\n",
    "1. ALGORITHM SELECTION GUIDELINES:\n",
    "   - prompt_memory: Production systems, real-time learning, cost-sensitive\n",
    "   - gradient: General purpose, balanced performance, failure analysis\n",
    "   - metaprompt: Research, complex domains, quality-critical applications\n",
    "\n",
    "2. HYBRID APPROACHES:\n",
    "   - Use prompt_memory for immediate updates\n",
    "   - Run gradient weekly for targeted improvements\n",
    "   - Apply metaprompt monthly for deep insights\n",
    "\n",
    "3. OPTIMIZATION TRIGGERS:\n",
    "   - Conversation volume threshold (e.g., every 100 conversations)\n",
    "   - Performance degradation (success rate drops below threshold)\n",
    "   - Scheduled intervals (hourly, daily, weekly)\n",
    "   - User feedback signals (low satisfaction scores)\n",
    "\n",
    "4. PRODUCTION PATTERNS:\n",
    "   - Start with prompt_memory for MVP/prototype\n",
    "   - Graduate to gradient as system matures\n",
    "   - Use metaprompt for periodic deep analysis\n",
    "   - Implement dynamic selection for optimal results\n",
    "\n",
    "5. MONITORING AND EVALUATION:\n",
    "   - Track optimization frequency and cost\n",
    "   - Measure performance improvement over time\n",
    "   - A/B test different algorithms\n",
    "   - Monitor user satisfaction changes\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)\n",
    "\n",
    "# Save final comparison report\n",
    "if 'comparison_results' in locals():\n",
    "    report_path = \"optimization_comparison_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        clean_results = {}\n",
    "        for key, value in comparison_results.items():\n",
    "            if isinstance(value, dict):\n",
    "                clean_results[key] = {}\n",
    "                for k, v in value.items():\n",
    "                    if isinstance(v, dict):\n",
    "                        clean_results[key][k] = {\n",
    "                            subk: float(subv) if isinstance(subv, np.number) else subv\n",
    "                            for subk, subv in v.items()\n",
    "                        }\n",
    "                    else:\n",
    "                        clean_results[key][k] = float(v) if isinstance(v, np.number) else v\n",
    "            else:\n",
    "                clean_results[key] = value\n",
    "        \n",
    "        json.dump(clean_results, f, indent=2)\n",
    "    print(f\"\\nComparison report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
