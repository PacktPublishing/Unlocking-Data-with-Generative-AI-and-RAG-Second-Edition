{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00bc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Dependencies\n",
    "%pip install langchain\n",
    "%pip install langgraph\n",
    "%pip install langchain-openai\n",
    "%pip install chromadb\n",
    "%pip install python-dotenv\n",
    "%pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60903289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import Core Libraries and Configure Environment\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, TypedDict, Annotated, Sequence\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='env.txt')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize models\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9a69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: \n",
    "# Agent State Definition\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Complete state container for multi-memory agent\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    working_memory: dict\n",
    "    episodic_recall: list\n",
    "    semantic_facts: dict\n",
    "    user_id: str\n",
    "    conversation_id: str\n",
    "\n",
    "# Semantic Fact Structure\n",
    "class SemanticFact(BaseModel):\n",
    "    \"\"\"Structure for semantic memory facts\"\"\"\n",
    "    subject: str = Field(description=\"Entity or topic\")\n",
    "    predicate: str = Field(description=\"Relationship or property\")\n",
    "    object: str = Field(description=\"Value or related entity\")\n",
    "    confidence: float = Field(description=\"Confidence score 0-1\")\n",
    "    source: str = Field(description=\"Source: user or assistant\")\n",
    "\n",
    "# Initialize Vector Store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"agent_memory\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./memory_store\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3032b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: \n",
    "def store_episodic_memory(vector_store, conversation_id: str, messages: List, summary: str = None):\n",
    "    \"\"\"Store conversation episode in vector memory\"\"\"\n",
    "    if not summary and messages:\n",
    "        summary = f\"Conversation about: {messages[0][1] if isinstance(messages[0], tuple) else messages[0].content[:100]}...\"\n",
    "    \n",
    "    metadata = {\n",
    "        \"type\": \"episodic\",\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"message_count\": len(messages)\n",
    "    }\n",
    "    \n",
    "    conversation_text = \"\"\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, tuple):\n",
    "            conversation_text += f\"{msg[0]}: {msg[1]}\\n\"\n",
    "        else:\n",
    "            conversation_text += f\"{msg.type}: {msg.content}\\n\"\n",
    "    \n",
    "    doc = Document(page_content=conversation_text, metadata=metadata)\n",
    "    vector_store.add_documents([doc])\n",
    "    return conversation_id\n",
    "\n",
    "def retrieve_episodic_memories(vector_store, query: str, k: int = 3):\n",
    "    \"\"\"Retrieve relevant past conversation episodes\"\"\"\n",
    "    results = vector_store.similarity_search(\n",
    "        query=query,\n",
    "        k=k,\n",
    "        filter={\"type\": {\"$eq\": \"episodic\"}}\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88a5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Semantic Memory Functions\n",
    "def extract_semantic_facts(messages: List) -> List[SemanticFact]:\n",
    "    \"\"\"Extract factual knowledge from conversation\"\"\"\n",
    "    extraction_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Analyze this conversation and extract important factual statements.\n",
    "    \n",
    "    Conversation: {conversation}\n",
    "    \n",
    "    Extract facts in JSON format:\n",
    "    {{\"facts\": [{{\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\", \n",
    "                  \"confidence\": 0.0-1.0, \"source\": \"user or assistant\"}}]}}\n",
    "    \n",
    "    Only extract clear facts. Output valid JSON only.\n",
    "    \"\"\")\n",
    "    \n",
    "    conversation_text = \"\"\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, tuple):\n",
    "            conversation_text += f\"{msg[0]}: {msg[1]}\\n\"\n",
    "        else:\n",
    "            conversation_text += f\"{msg.type}: {msg.content}\\n\"\n",
    "    \n",
    "    parser = JsonOutputParser()\n",
    "    chain = extraction_prompt | llm | parser\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke({\"conversation\": conversation_text})\n",
    "        facts = [SemanticFact(**fact_dict) for fact_dict in result.get(\"facts\", [])]\n",
    "        return facts\n",
    "    except Exception as e:\n",
    "        print(f\"Fact extraction error: {e}\")\n",
    "        return []\n",
    "\n",
    "def store_semantic_facts(vector_store, facts: List[SemanticFact], user_id: str = \"default\"):\n",
    "    \"\"\"Store semantic facts in vector memory\"\"\"\n",
    "    documents = []\n",
    "    for fact in facts:\n",
    "        fact_text = f\"{fact.subject} {fact.predicate} {fact.object}\"\n",
    "        metadata = {\n",
    "            \"type\": \"semantic\",\n",
    "            \"user_id\": user_id,\n",
    "            \"subject\": fact.subject,\n",
    "            \"predicate\": fact.predicate,\n",
    "            \"object\": fact.object,\n",
    "            \"confidence\": fact.confidence,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        doc = Document(page_content=fact_text, metadata=metadata)\n",
    "        documents.append(doc)\n",
    "    \n",
    "    if documents:\n",
    "        vector_store.add_documents(documents)\n",
    "    return len(documents)\n",
    "\n",
    "def retrieve_semantic_facts(vector_store, query: str, user_id: str = \"default\", k: int = 5):\n",
    "    \"\"\"Retrieve relevant semantic facts\"\"\"\n",
    "    results = vector_store.similarity_search(\n",
    "        query=query, k=k,\n",
    "        filter={\"$and\": [\n",
    "            {\"type\": {\"$eq\": \"semantic\"}},\n",
    "            {\"user_id\": {\"$eq\": user_id}}\n",
    "        ]}\n",
    "    )\n",
    "    \n",
    "    facts = []\n",
    "    for doc in results:\n",
    "        facts.append({\n",
    "            \"subject\": doc.metadata.get(\"subject\"),\n",
    "            \"predicate\": doc.metadata.get(\"predicate\"),\n",
    "            \"object\": doc.metadata.get(\"object\"),\n",
    "            \"confidence\": doc.metadata.get(\"confidence\", 1.0)\n",
    "        })\n",
    "    return facts\n",
    "\n",
    "def format_semantic_context(facts: List[Dict]) -> str:\n",
    "    \"\"\"Format semantic facts for prompt inclusion\"\"\"\n",
    "    if not facts:\n",
    "        return \"No relevant facts found.\"\n",
    "    \n",
    "    context = \"Known facts:\\n\"\n",
    "    for fact in facts:\n",
    "        if fact.get('confidence', 1.0) > 0.7:\n",
    "            context += f\"- {fact['subject']} {fact['predicate']} {fact['object']}\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd994523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Unified Memory Agent\n",
    "def unified_memory_agent(state: AgentState) -> dict:\n",
    "    \"\"\"Agent with episodic and semantic memory capabilities\"\"\"\n",
    "    \n",
    "    current_messages = state.get(\"messages\", [])\n",
    "    user_id = state.get(\"user_id\", \"default\")\n",
    "    conversation_id = state.get(\"conversation_id\", f\"conv_{datetime.now().timestamp()}\")\n",
    "    \n",
    "    # Retrieve context from both memory types\n",
    "    episodic_context = \"\"\n",
    "    semantic_context = \"\"\n",
    "    \n",
    "    if current_messages:\n",
    "        # Get latest query\n",
    "        if isinstance(current_messages[-1], tuple):\n",
    "            latest_query = current_messages[-1][1]\n",
    "        else:\n",
    "            latest_query = current_messages[-1].content\n",
    "        \n",
    "        # Retrieve episodic memories\n",
    "        past_episodes = retrieve_episodic_memories(vector_store, latest_query, k=2)\n",
    "        if past_episodes:\n",
    "            episodic_context = \"Relevant past conversations:\\n\"\n",
    "            for episode in past_episodes:\n",
    "                timestamp = episode.metadata.get('timestamp', 'Unknown')\n",
    "                episodic_context += f\"[{timestamp}]:\\n{episode.page_content[:200]}...\\n\\n\"\n",
    "        \n",
    "        # Retrieve semantic facts\n",
    "        facts = retrieve_semantic_facts(vector_store, latest_query, user_id=user_id, k=3)\n",
    "        semantic_context = format_semantic_context(facts)\n",
    "    \n",
    "    # Create memory-augmented prompt\n",
    "    memory_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant with both episodic and semantic memory.\n",
    "\n",
    "{semantic_context}\n",
    "\n",
    "{episodic_context}\n",
    "\n",
    "Current conversation:\n",
    "{messages}\n",
    "\n",
    "Respond using your memories when relevant. Be consistent with known facts and past conversations.\n",
    "\"\"\")\n",
    "    \n",
    "    # Format current messages\n",
    "    formatted_messages = \"\"\n",
    "    for msg in current_messages[-5:] if current_messages else []:\n",
    "        if isinstance(msg, tuple):\n",
    "            formatted_messages += f\"{msg[0]}: {msg[1]}\\n\"\n",
    "        else:\n",
    "            formatted_messages += f\"{msg.type}: {msg.content}\\n\"\n",
    "    \n",
    "    # Generate response\n",
    "    chain = memory_prompt | llm | output_parser\n",
    "    response = chain.invoke({\n",
    "        \"semantic_context\": semantic_context,\n",
    "        \"episodic_context\": episodic_context,\n",
    "        \"messages\": formatted_messages\n",
    "    })\n",
    "    \n",
    "    # Store conversation as episodic memory (after 2+ exchanges)\n",
    "    if len(current_messages) >= 2:\n",
    "        store_episodic_memory(vector_store, conversation_id, current_messages)\n",
    "    \n",
    "    # Extract and store new semantic facts\n",
    "    if current_messages:\n",
    "        messages_with_response = current_messages + [(\"assistant\", response)]\n",
    "        new_facts = extract_semantic_facts(messages_with_response[-3:])\n",
    "        if new_facts:\n",
    "            stored = store_semantic_facts(vector_store, new_facts, user_id)\n",
    "            state[\"semantic_facts\"] = {\"extracted\": stored}\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [(\"assistant\", response)],\n",
    "        \"episodic_recall\": past_episodes if past_episodes else [],\n",
    "        \"semantic_facts\": state.get(\"semantic_facts\", {})\n",
    "    }\n",
    "\n",
    "# Build workflow\n",
    "memory_workflow = StateGraph(AgentState)\n",
    "memory_workflow.add_node(\"memory_agent\", unified_memory_agent)\n",
    "memory_workflow.set_entry_point(\"memory_agent\")\n",
    "memory_workflow.add_edge(\"memory_agent\", END)\n",
    "\n",
    "# Compile application\n",
    "memory_app = memory_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8e7cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Hi Sarah! Great to meet you. As a data scientist working on climate models, I can provide concise technical explanations tailored to your work. If you have any specific questions or topics you'd like to discuss—whether about climate modeling techniques, data processing, or anything else—just let me know. Also, if you want any vegetarian-friendly recommendations or tips related to your preferences, feel free to ask!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Response 2 (with semantic memory):\n",
      "For time series forecasting, I recommend these machine learning techniques:\n",
      "\n",
      "1. **ARIMA/SARIMA** – Traditional statistical models effective for linear patterns and seasonality.\n",
      "2. **LSTM (Long Short-Term Memory networks)** – Deep learning models that capture long-term dependencies in sequential data.\n",
      "3. **Prophet** – A decomposable time series model by Facebook, good for handling seasonality and holidays with minimal tuning.\n",
      "4. **XGBoost/LightGBM** – Gradient boosting methods that work well when you engineer time-based features (lags, rolling stats).\n",
      "5. **Temporal Convolutional Networks (TCN)** – Convolutional models designed for sequence data, often faster to train than RNNs.\n",
      "\n",
      "Given your background in climate models, LSTM or TCN could be particularly useful for capturing complex temporal dependencies in environmental data. Let me know if you want a concise technical summary of any specific method.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Response 3 (with episodic and semantic memory):\n",
      "Since Sarah Chen is vegetarian and prefers concise technical explanations, here are some lunch options that cater to her preferences and can work well for the team meeting:\n",
      "\n",
      "1. **Vegetarian grain bowls** – Balanced, nutrient-dense, and easy to customize.\n",
      "2. **Mediterranean mezze platters** – Hummus, falafel, tabbouleh, and pita.\n",
      "3. **Caprese sandwiches** – Fresh mozzarella, tomato, basil on ciabatta.\n",
      "4. **Vegetable sushi rolls** – Light, portable, and vegetarian-friendly.\n",
      "5. **Salad jars** – Layered salads with quinoa, chickpeas, and veggies.\n",
      "\n",
      "Would you like me to suggest specific local restaurants or catering services?\n"
     ]
    }
   ],
   "source": [
    "# Test conversation 1: Establish facts\n",
    "test_1 = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Hi, I'm Sarah Chen. I'm a data scientist working on climate models. I'm vegetarian and prefer concise technical explanations.\"),\n",
    "    ],\n",
    "    \"user_id\": \"sarah_chen\",\n",
    "    \"conversation_id\": \"conv_001\"\n",
    "}\n",
    "\n",
    "result_1 = memory_app.invoke(test_1)\n",
    "print(\"Response 1:\")\n",
    "print(result_1[\"messages\"][-1][1] if isinstance(result_1[\"messages\"][-1], tuple) else result_1[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test conversation 2: Use semantic memory\n",
    "test_2 = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What machine learning techniques would you recommend for time series forecasting?\"),\n",
    "    ],\n",
    "    \"user_id\": \"sarah_chen\",\n",
    "    \"conversation_id\": \"conv_002\"\n",
    "}\n",
    "\n",
    "result_2 = memory_app.invoke(test_2)\n",
    "print(\"Response 2 (with semantic memory):\")\n",
    "print(result_2[\"messages\"][-1][1] if isinstance(result_2[\"messages\"][-1], tuple) else result_2[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test conversation 3: Reference past conversation\n",
    "test_3 = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Can you recommend some lunch options for our team meeting?\"),\n",
    "    ],\n",
    "    \"user_id\": \"sarah_chen\",\n",
    "    \"conversation_id\": \"conv_003\"\n",
    "}\n",
    "\n",
    "result_3 = memory_app.invoke(test_3)\n",
    "print(\"Response 3 (with episodic and semantic memory):\")\n",
    "print(result_3[\"messages\"][-1][1] if isinstance(result_3[\"messages\"][-1], tuple) else result_3[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
