{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Install necessary libraries\n",
    "%pip install -q chromadb\n",
    "%pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5381a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "COLLECTION_NAME = \"semantic_cache\"\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Clean start when running all cells\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(\"âœ“ Cleared existing semantic_cache collection\")\n",
    "except Exception:\n",
    "    print(\"âœ“ No existing collection to clear\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "def _new_id():\n",
    "    return str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Cache Entry Structure\n",
    "class SemanticCache:\n",
    "    def __init__(self, embedder_model='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        \"\"\"Initialize embedding model and reuse the shared ChromaDB collection.\"\"\"\n",
    "        self.embedder = SentenceTransformer(embedder_model)\n",
    "        self.collection = collection_ref if collection_ref is not None else collection\n",
    "\n",
    "    def add(self, query, soln_path):\n",
    "        \"\"\"Add query-solution path pair to cache\"\"\"\n",
    "        embedding = self.embedder.encode(query).tolist()\n",
    "        self.collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[query],\n",
    "            metadatas=[{'query': query, 'soln_path': soln_path}],\n",
    "            ids=[_new_id()]\n",
    "        )\n",
    "\n",
    "    def search(self, query, threshold=0.75):\n",
    "        \"\"\"Search for similar cached query\"\"\"\n",
    "        embedding = self.embedder.encode(query).tolist()\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=1)\n",
    "        \n",
    "        if results.get('distances') and results['distances'][0]:\n",
    "            score = 1 - results['distances'][0][0]\n",
    "            if score >= threshold:\n",
    "                metadata = results['metadatas'][0][0]\n",
    "                return {\n",
    "                    'soln_path': metadata['soln_path'],\n",
    "                    'score': score,\n",
    "                    'cached_query': metadata.get('query')\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Test it\n",
    "cache = SemanticCache()\n",
    "cache.add(\"What is the capital of France?\", \"lookup_fact('France', 'capital')\")\n",
    "cache.add(\"How do I reset my password?\", \"get_help_article('password_reset')\")\n",
    "cache.add(\"What are your business hours?\", \"get_business_info('hours')\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What's the capital of France?\",\n",
    "    \"Password reset instructions\",\n",
    "    \"When are you open?\",\n",
    "    \"What's the weather today?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    # Get the raw similarity score even if below threshold\n",
    "    embedding = cache.embedder.encode(q).tolist()\n",
    "    results = cache.collection.query(query_embeddings=[embedding], n_results=1)\n",
    "    \n",
    "    if results.get('distances') and results['distances'][0]:\n",
    "        score = 1 - results['distances'][0][0]\n",
    "        \n",
    "        # Now check against threshold\n",
    "        result = cache.search(q)\n",
    "        if result:\n",
    "            print(f\"âœ… '{q}' â†’ '{result['soln_path']}' (score: {result['score']:.2f})\")\n",
    "        else:\n",
    "            print(f\"âŒ '{q}' â†’ Match below threshold (score: {score:.2f})\")\n",
    "    else:\n",
    "        print(f\"âŒ '{q}' â†’ No match (no cached queries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Semantic Boundaries\n",
    "class MaskedSemanticCache(SemanticCache):\n",
    "    def mask_entities(self, text):\n",
    "        \"\"\"Replace specific entities with placeholders\"\"\"\n",
    "        text = re.sub(r'\\$[\\d,]+', '[AMOUNT]', text)           # Money amounts\n",
    "        text = re.sub(r'\\b[A-Z]{2,5}\\b', '[TICKER]', text)     # Tickers\n",
    "        text = re.sub(r'\\b20\\d{2}\\b', '[YEAR]', text)          # Years\n",
    "        text = re.sub(r'\\d+(\\.\\d+)?%', '[PERCENT]', text)      # Percentages\n",
    "        text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)             # Emails\n",
    "        return text\n",
    "\n",
    "    def add(self, query, soln_path):\n",
    "        \"\"\"Add with entity masking\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        self.collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[masked_query],\n",
    "            metadatas=[{\n",
    "                'original_query': query,\n",
    "                'masked_query': masked_query,\n",
    "                'soln_path': soln_path\n",
    "            }],\n",
    "            ids=[_new_id()]\n",
    "        )\n",
    "\n",
    "    def search(self, query, threshold=0.75):\n",
    "        \"\"\"Search using masked query\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "        if results.get('distances') and results['distances'][0]:\n",
    "            score = 1 - results['distances'][0][0]\n",
    "            if score >= threshold:\n",
    "                metadata = results['metadatas'][0][0]\n",
    "                return {\n",
    "                    'soln_path': metadata['soln_path'],\n",
    "                    'score': score,\n",
    "                    'cached_query': metadata.get('original_query'),\n",
    "                    'masked_query': metadata.get('masked_query')\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Test entity masking\n",
    "cache = MaskedSemanticCache()\n",
    "cache.add(\"What was AAPL stock price in 2023?\", \"stock_price_tool\")\n",
    "cache.add(\"My budget is $5000\", \"budget_tool\")\n",
    "\n",
    "print(\"Testing entity masking:\")\n",
    "result = cache.search(\"What was TSLA stock price in 2024?\")\n",
    "if result:\n",
    "    print(f\"âœ… Matched despite different ticker and year!\")\n",
    "    print(f\"   Original: {result['cached_query']}\")\n",
    "    print(f\"   Masked: {result['masked_query']}\")\n",
    "    print(f\"   Solution path: {result['soln_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Cross-Encoder Verification\n",
    "class CrossEncoderSemanticCache(MaskedSemanticCache):\n",
    "    def __init__(self, embedder_model='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(embedder_model=embedder_model, collection_ref=collection_ref)\n",
    "        self.verifier = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    def search_with_verification(self, query, vector_threshold=0.7, verify_threshold=3.5):\n",
    "        \"\"\"Two-stage search: vector similarity + verification\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=3)\n",
    "        if not (results.get('distances') and results['distances'][0]):\n",
    "            return None\n",
    "\n",
    "        best_match, best_score = None, 0.0\n",
    "        for i, distance in enumerate(results['distances'][0]):\n",
    "            vector_score = 1 - distance\n",
    "            if vector_score < vector_threshold:\n",
    "                continue\n",
    "\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            verify_score = float(self.verifier.predict(\n",
    "                [[query, metadata.get('original_query', metadata.get('query', ''))]]\n",
    "            )[0])\n",
    "            if verify_score > best_score and verify_score >= verify_threshold:\n",
    "                best_score = verify_score\n",
    "                best_match = {\n",
    "                    'soln_path': metadata['soln_path'],\n",
    "                    'vector_score': vector_score,\n",
    "                    'verify_score': verify_score,\n",
    "                    'cached_query': metadata.get('original_query', metadata.get('query'))\n",
    "                }\n",
    "        return best_match\n",
    "\n",
    "# Test verification\n",
    "cache = CrossEncoderSemanticCache()\n",
    "cache.add(\"What is my checking account balance?\", \"checking_balance_tool\")\n",
    "cache.add(\"What is my savings account balance?\", \"savings_balance_tool\")\n",
    "cache.add(\"What is my credit card balance?\", \"credit_balance_tool\")\n",
    "\n",
    "queries = [\n",
    "    \"What's my checking balance\",\n",
    "    \"What's my savings account balance?\",\n",
    "    \"Credit card balance\"\n",
    "]\n",
    "\n",
    "print(\"Testing with cross-encoder verification:\")\n",
    "for q in queries:\n",
    "    result = cache.search_with_verification(q)\n",
    "    if result:\n",
    "        print(f\"âœ… '{q}' â†’ '{result['soln_path']}'\")\n",
    "        print(f\"   Vector: {result['vector_score']:.2f}, Verified: {result['verify_score']:.2f}\")\n",
    "    else:\n",
    "        print(f\"âŒ '{q}' â†’ No verified match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b615268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Adaptive Thresholds\n",
    "class AdaptiveSemanticCache(CrossEncoderSemanticCache):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(embedder_model=model_name, collection_ref=collection_ref)\n",
    "        self.model_name = model_name\n",
    "        self.model_thresholds = {\n",
    "            'all-MiniLM-L6-v2': 0.75,\n",
    "            'all-mpnet-base-v2': 0.80,\n",
    "            'all-distilroberta-v1': 0.70\n",
    "        }\n",
    "\n",
    "    def get_threshold(self, match_type='normal'):\n",
    "        base = self.model_thresholds.get(self.model_name, 0.75)\n",
    "        adjustments = {\n",
    "            'exact': base + 0.15,\n",
    "            'normal': base,\n",
    "            'fuzzy': base - 0.10,\n",
    "            'exploratory': base - 0.20\n",
    "        }\n",
    "        return adjustments.get(match_type, base)\n",
    "\n",
    "    def adaptive_search(self, query, match_type='normal'):\n",
    "        threshold = self.get_threshold(match_type)\n",
    "        verify_threshold = 0.9 if match_type == 'exact' else 0.85\n",
    "        return self.search_with_verification(\n",
    "            query,\n",
    "            vector_threshold=threshold,\n",
    "            verify_threshold=verify_threshold\n",
    "        )\n",
    "\n",
    "# Test adaptive thresholds with fallback demonstration\n",
    "cache = AdaptiveSemanticCache()\n",
    "cache.add(\"What is the annual revenue?\", \"revenue_tool\")\n",
    "cache.add(\"Show me customer demographics\", \"demographics_tool\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"yearly revenue\", \"Strong match\"),\n",
    "    (\"customer demographic\", \"Weaker match\")\n",
    "]\n",
    "\n",
    "for query, description in test_cases:\n",
    "    print(f\"\\nQuery: '{query}' ({description})\")\n",
    "    for match_type in ['exact', 'normal', 'fuzzy']:\n",
    "        result = cache.adaptive_search(query, match_type)\n",
    "        threshold = cache.get_threshold(match_type)\n",
    "        if result:\n",
    "            print(f\"  {match_type.upper()} (threshold {threshold:.2f}): âœ… Found match\")\n",
    "        else:\n",
    "            print(f\"  {match_type.upper()} (threshold {threshold:.2f}): âŒ No match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Auto-Population and Statistics\n",
    "class SmartSemanticCache(AdaptiveSemanticCache):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(model_name=model_name, collection_ref=collection_ref)\n",
    "        self.stats = {'hits': 0, 'misses': 0, 'auto_added': 0}\n",
    "\n",
    "    def query_with_fallback(self, query, fallback_fn=None, match_type='normal'):\n",
    "        \"\"\"Try cache first, fallback to function if miss\"\"\"\n",
    "        result = self.adaptive_search(query, match_type)\n",
    "        if result:\n",
    "            self.stats['hits'] += 1\n",
    "            return result['soln_path'], 'cache'\n",
    "\n",
    "        self.stats['misses'] += 1\n",
    "        if fallback_fn:\n",
    "            soln_path = fallback_fn(query)\n",
    "            self.add(query, soln_path)\n",
    "            self.stats['auto_added'] += 1\n",
    "            return soln_path, 'computed'\n",
    "\n",
    "        return None, 'miss'\n",
    "\n",
    "    def print_stats(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        if total > 0:\n",
    "            hit_rate = self.stats['hits'] / total * 100\n",
    "            print(\"Cache Stats:\")\n",
    "            print(f\"  Hits: {self.stats['hits']} ({hit_rate:.1f}%)\")\n",
    "            print(f\"  Misses: {self.stats['misses']}\")\n",
    "            print(f\"  Auto-added: {self.stats['auto_added']}\")\n",
    "\n",
    "# Mock agent function\n",
    "def mock_agent(query):\n",
    "    \"\"\"Simulate an expensive agent call\"\"\"\n",
    "    q = query.lower()\n",
    "    if 'checking' in q and 'balance' in q:\n",
    "        return 'checking_balance_tool'\n",
    "    elif 'savings' in q and 'balance' in q:\n",
    "        return 'savings_balance_tool'\n",
    "    elif ('credit' in q or 'card' in q) and 'balance' in q:\n",
    "        return 'credit_balance_tool'\n",
    "    elif 'balance' in q:\n",
    "        return 'balance_tool'  # Generic balance query\n",
    "    elif 'transaction' in q:\n",
    "        return 'transaction_tool'\n",
    "    else:\n",
    "        return 'general_tool'\n",
    "\n",
    "# Test with fallback\n",
    "cache = SmartSemanticCache()\n",
    "queries = [\n",
    "    \"What is my account balance?\",\n",
    "    \"Show me my account balance\",\n",
    "    \"Account balance please\",\n",
    "    \"Recent transactions\",\n",
    "    \"Show my transactions\",\n",
    "]\n",
    "\n",
    "print(\"Testing with auto-population:\")\n",
    "for q in queries:\n",
    "    soln_path, source = cache.query_with_fallback(q, mock_agent)\n",
    "    print(f\"'{q}' â†’ {soln_path} ({source})\")\n",
    "\n",
    "print()\n",
    "cache.print_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64672a6a",
   "metadata": {},
   "source": [
    "---\n",
    "**ðŸ’¡ BONUS STEP 8: Integrating Semantic Cache with Planning Agents**\n",
    "\n",
    "This bonus step demonstrates how to integrate the semantic cache into a complete agentic system. In production RAG applications, you typically have a planning agent that orchestrates tool calls and multi-step workflows. The semantic cache acts as a fast pathâ€”if a query matches a cached solution path, you skip expensive planning and go straight to execution.\n",
    "\n",
    "This pattern is particularly valuable for:\n",
    "- **Reducing latency**: Cache hits bypass the planning agent entirely\n",
    "- **Cost savings**: Every cached query avoids an LLM call for planning\n",
    "- **Consistency**: Proven solution paths get reused automatically\n",
    "\n",
    "The `CacheWithFallback` class shows the integration pattern: try the cache first with cross-encoder verification, and only invoke the planning agent on cache misses. When the planning agent determines a solution path, it's automatically added to the cache for future queries.\n",
    "\n",
    "**How to use this in production:**\n",
    "1. Replace `mock_planning_agent()` with your actual planning agent (LLM-based router, chain orchestrator, etc.)\n",
    "2. Adjust the cache implementation (`cache_impl`) to use the appropriate verification and threshold settings for your use case\n",
    "3. Monitor hit rates and adjust thresholds based on false positive/negative rates\n",
    "4. Consider adding cache invalidation logic if your tools or solution paths change\n",
    "\n",
    "**ðŸ“ Try this:** Run the cell below once to see the cache learning from misses, then run it again to see how the hit rate improves to 100% as the cache serves all queries instantly!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67483fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 (Bonus step!): Fallback to Planning Agent\n",
    "class CacheWithFallback:\n",
    "    def __init__(self, cache_impl=None):\n",
    "        # Reuse the verified cache (or inject a smarter one)\n",
    "        self.cache = cache_impl if cache_impl is not None else CrossEncoderSemanticCache()\n",
    "        self.stats = {'hits': 0, 'misses': 0}\n",
    "\n",
    "    def process_query(self, query):\n",
    "        \"\"\"Try cache first, then fallback to agent\"\"\"\n",
    "        result = self.cache.search_with_verification(query)\n",
    "        if result:\n",
    "            self.stats['hits'] += 1\n",
    "            print(f\"âœ“ Cache hit: {result['soln_path']}\")\n",
    "            return {**result, 'from_cache': True}\n",
    "\n",
    "        self.stats['misses'] += 1\n",
    "        print(\"âœ— Cache miss - calling planning agent...\")\n",
    "\n",
    "        soln_path = self.mock_planning_agent(query)\n",
    "\n",
    "        if soln_path:\n",
    "            self.cache.add(query, soln_path)\n",
    "            print(f\"â†’ Added to cache: {soln_path}\")\n",
    "\n",
    "        return {'soln_path': soln_path, 'from_cache': False}\n",
    "\n",
    "    def mock_planning_agent(self, query):\n",
    "        \"\"\"Simulate a planning agent (replace with real agent)\"\"\"\n",
    "        q = query.lower()\n",
    "        if 'checking' in q and 'balance' in q:\n",
    "            return 'checking_balance_tool'\n",
    "        elif 'savings' in q and 'balance' in q:\n",
    "            return 'savings_balance_tool'\n",
    "        elif ('credit' in q or 'card' in q) and 'balance' in q:\n",
    "            return 'credit_balance_tool'\n",
    "        elif 'balance' in q:\n",
    "            return 'balance_tool'\n",
    "        elif 'transaction' in q:\n",
    "            return 'transaction_tool'\n",
    "        elif 'stock' in q:\n",
    "            return 'stock_tool'\n",
    "        else:\n",
    "            return 'general_tool'\n",
    "\n",
    "    def print_stats(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = (self.stats['hits'] / total) if total > 0 else 0\n",
    "        print(f\"\\nStats: {self.stats['hits']} hits, {self.stats['misses']} misses\")\n",
    "        print(f\"Hit rate: {hit_rate:.1%}\")\n",
    "\n",
    "# Test the full system\n",
    "system = CacheWithFallback()\n",
    "\n",
    "queries = [\n",
    "    \"What is my account balance?\",\n",
    "    \"Show me my account balance\",\n",
    "    \"What are my recent transactions?\",\n",
    "    \"Display my transactions\",\n",
    "    \"What is my balance?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    system.process_query(q)\n",
    "\n",
    "system.print_stats()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
