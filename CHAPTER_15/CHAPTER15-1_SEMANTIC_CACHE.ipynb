{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3aa21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# step 1: Install necessary libraries\n",
    "%pip install -q chromadb\n",
    "%pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5381a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No existing collection to clear\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "COLLECTION_NAME = \"semantic_cache\"\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Clean start when running all cells\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(\"✓ Cleared existing semantic_cache collection\")\n",
    "except Exception:\n",
    "    print(\"✓ No existing collection to clear\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "def _new_id():\n",
    "    return str(uuid.uuid4())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0576a469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'What's the capital of France?' → 'lookup_fact('France', 'capital')' (score: 0.99)\n",
      "✅ 'Password reset instructions' → 'get_help_article('password_reset')' (score: 0.80)\n",
      "❌ 'When are you open?' → Match below threshold (score: 0.51)\n",
      "❌ 'What's the weather today?' → Match below threshold (score: 0.27)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Cache Entry Structure\n",
    "class SemanticCache:\n",
    "    def __init__(self, embedder_model='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        \"\"\"Initialize embedding model and reuse the shared ChromaDB collection.\"\"\"\n",
    "        self.embedder = SentenceTransformer(embedder_model)\n",
    "        self.collection = collection_ref if collection_ref is not None else collection\n",
    "\n",
    "    def add(self, query, soln_path):\n",
    "        \"\"\"Add query-solution path pair to cache\"\"\"\n",
    "        embedding = self.embedder.encode(query).tolist()\n",
    "        self.collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[query],\n",
    "            metadatas=[{'query': query, 'soln_path': soln_path}],\n",
    "            ids=[_new_id()]\n",
    "        )\n",
    "\n",
    "    def search(self, query, threshold=0.75):\n",
    "        \"\"\"Search for similar cached query\"\"\"\n",
    "        embedding = self.embedder.encode(query).tolist()\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=1)\n",
    "        \n",
    "        if results.get('distances') and results['distances'][0]:\n",
    "            score = 1 - results['distances'][0][0]\n",
    "            if score >= threshold:\n",
    "                metadata = results['metadatas'][0][0]\n",
    "                return {\n",
    "                    'soln_path': metadata['soln_path'],\n",
    "                    'score': score,\n",
    "                    'cached_query': metadata.get('query')\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Test it\n",
    "cache = SemanticCache()\n",
    "cache.add(\"What is the capital of France?\", \"lookup_fact('France', 'capital')\")\n",
    "cache.add(\"How do I reset my password?\", \"get_help_article('password_reset')\")\n",
    "cache.add(\"What are your business hours?\", \"get_business_info('hours')\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What's the capital of France?\",\n",
    "    \"Password reset instructions\",\n",
    "    \"When are you open?\",\n",
    "    \"What's the weather today?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    # Get the raw similarity score even if below threshold\n",
    "    embedding = cache.embedder.encode(q).tolist()\n",
    "    results = cache.collection.query(query_embeddings=[embedding], n_results=1)\n",
    "    \n",
    "    if results.get('distances') and results['distances'][0]:\n",
    "        score = 1 - results['distances'][0][0]\n",
    "        \n",
    "        # Now check against threshold\n",
    "        result = cache.search(q)\n",
    "        if result:\n",
    "            print(f\"✅ '{q}' → '{result['soln_path']}' (score: {result['score']:.2f})\")\n",
    "        else:\n",
    "            print(f\"❌ '{q}' → Match below threshold (score: {score:.2f})\")\n",
    "    else:\n",
    "        print(f\"❌ '{q}' → No match (no cached queries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bb3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing entity masking:\n",
      "✅ Matched despite different ticker and year!\n",
      "   Original: What was AAPL stock price in 2023?\n",
      "   Masked: What was [TICKER] stock price in [YEAR]?\n",
      "   Response: Use stock_price_tool\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Semantic Boundaries\n",
    "class MaskedSemanticCache(SemanticCache):\n",
    "    def mask_entities(self, text):\n",
    "        \"\"\"Replace specific entities with placeholders\"\"\"\n",
    "        text = re.sub(r'\\$[\\d,]+', '[AMOUNT]', text)           # Money amounts\n",
    "        text = re.sub(r'\\b[A-Z]{2,5}\\b', '[TICKER]', text)     # Tickers\n",
    "        text = re.sub(r'\\b20\\d{2}\\b', '[YEAR]', text)          # Years\n",
    "        text = re.sub(r'\\d+(\\.\\d+)?%', '[PERCENT]', text)      # Percentages\n",
    "        text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)             # Emails\n",
    "        return text\n",
    "\n",
    "    def add(self, query, response):\n",
    "        \"\"\"Add with entity masking\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        self.collection.add(\n",
    "            embeddings=[embedding],\n",
    "            documents=[masked_query],\n",
    "            metadatas=[{\n",
    "                'original_query': query,\n",
    "                'masked_query': masked_query,\n",
    "                'response': response\n",
    "            }],\n",
    "            ids=[_new_id()]\n",
    "        )\n",
    "\n",
    "    def search(self, query, threshold=0.75):\n",
    "        \"\"\"Search using masked query\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "        if results.get('distances') and results['distances'][0]:\n",
    "            score = 1 - results['distances'][0][0]\n",
    "            if score >= threshold:\n",
    "                metadata = results['metadatas'][0][0]\n",
    "                return {\n",
    "                    'response': metadata['response'],\n",
    "                    'score': score,\n",
    "                    'cached_query': metadata.get('original_query'),\n",
    "                    'masked_query': metadata.get('masked_query')\n",
    "                }\n",
    "        return None\n",
    "\n",
    "# Test entity masking\n",
    "cache = MaskedSemanticCache()\n",
    "cache.add(\"What was AAPL stock price in 2023?\", \"Use stock_price_tool\")\n",
    "cache.add(\"My budget is $5000\", \"Use budget_tool\")\n",
    "\n",
    "print(\"Testing entity masking:\")\n",
    "result = cache.search(\"What was TSLA stock price in 2024?\")\n",
    "if result:\n",
    "    print(f\"✅ Matched despite different ticker and year!\")\n",
    "    print(f\"   Original: {result['cached_query']}\")\n",
    "    print(f\"   Masked: {result['masked_query']}\")\n",
    "    print(f\"   Response: {result['response']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50b78ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with cross-encoder verification:\n",
      "✅ 'What's my checking balance' → 'checking_balance_tool'\n",
      "   Vector: 0.89, Verified: 4.82\n",
      "✅ 'What's my savings account balance?' → 'savings_balance_tool'\n",
      "   Vector: 0.99, Verified: 6.35\n",
      "✅ 'Credit card balance' → 'credit_balance_tool'\n",
      "   Vector: 0.88, Verified: 4.01\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Cross-Encoder Verification\n",
    "class CrossEncoderSemanticCache(MaskedSemanticCache):\n",
    "    def __init__(self, embedder_model='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(embedder_model=embedder_model, collection_ref=collection_ref)\n",
    "        self.verifier = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    def search_with_verification(self, query, vector_threshold=0.7, verify_threshold=3.5):\n",
    "        \"\"\"Two-stage search: vector similarity + verification\"\"\"\n",
    "        masked_query = self.mask_entities(query)\n",
    "        embedding = self.embedder.encode(masked_query).tolist()\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=3)\n",
    "        if not (results.get('distances') and results['distances'][0]):\n",
    "            return None\n",
    "\n",
    "        best_match, best_score = None, 0.0\n",
    "        for i, distance in enumerate(results['distances'][0]):\n",
    "            vector_score = 1 - distance\n",
    "            if vector_score < vector_threshold:\n",
    "                continue\n",
    "\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            verify_score = float(self.verifier.predict(\n",
    "                [[query, metadata.get('original_query', metadata.get('query', ''))]]\n",
    "            )[0])\n",
    "            if verify_score > best_score and verify_score >= verify_threshold:\n",
    "                best_score = verify_score\n",
    "                best_match = {\n",
    "                    'response': metadata['response'],\n",
    "                    'vector_score': vector_score,\n",
    "                    'verify_score': verify_score,\n",
    "                    'cached_query': metadata.get('original_query', metadata.get('query'))\n",
    "                }\n",
    "        return best_match\n",
    "\n",
    "# Test verification\n",
    "cache = CrossEncoderSemanticCache()\n",
    "cache.add(\"What is my checking account balance?\", \"checking_balance_tool\")\n",
    "cache.add(\"What is my savings account balance?\", \"savings_balance_tool\")\n",
    "cache.add(\"What is my credit card balance?\", \"credit_balance_tool\")\n",
    "\n",
    "queries = [\n",
    "    \"What's my checking balance\",\n",
    "    \"What's my savings account balance?\",\n",
    "    \"Credit card balance\"\n",
    "]\n",
    "\n",
    "print(\"Testing with cross-encoder verification:\")\n",
    "for q in queries:\n",
    "    result = cache.search_with_verification(q)\n",
    "    if result:\n",
    "        print(f\"✅ '{q}' → '{result['response']}'\")\n",
    "        print(f\"   Vector: {result['vector_score']:.2f}, Verified: {result['verify_score']:.2f}\")\n",
    "    else:\n",
    "        print(f\"❌ '{q}' → No verified match found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b615268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'yearly revenue' (Strong match)\n",
      "  EXACT (threshold 0.90): ✅ Found match\n",
      "  NORMAL (threshold 0.75): ✅ Found match\n",
      "  FUZZY (threshold 0.65): ✅ Found match\n",
      "\n",
      "Query: 'customer demographic' (Weaker match)\n",
      "  EXACT (threshold 0.90): ❌ No match\n",
      "  NORMAL (threshold 0.75): ✅ Found match\n",
      "  FUZZY (threshold 0.65): ✅ Found match\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Adaptive Thresholds\n",
    "class AdaptiveSemanticCache(CrossEncoderSemanticCache):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(embedder_model=model_name, collection_ref=collection_ref)\n",
    "        self.model_name = model_name\n",
    "        self.model_thresholds = {\n",
    "            'all-MiniLM-L6-v2': 0.75,\n",
    "            'all-mpnet-base-v2': 0.80,\n",
    "            'all-distilroberta-v1': 0.70\n",
    "        }\n",
    "\n",
    "    def get_threshold(self, match_type='normal'):\n",
    "        base = self.model_thresholds.get(self.model_name, 0.75)\n",
    "        adjustments = {\n",
    "            'exact': base + 0.15,\n",
    "            'normal': base,\n",
    "            'fuzzy': base - 0.10,\n",
    "            'exploratory': base - 0.20\n",
    "        }\n",
    "        return adjustments.get(match_type, base)\n",
    "\n",
    "    def adaptive_search(self, query, match_type='normal'):\n",
    "        threshold = self.get_threshold(match_type)\n",
    "        verify_threshold = 0.9 if match_type == 'exact' else 0.85\n",
    "        return self.search_with_verification(\n",
    "            query,\n",
    "            vector_threshold=threshold,\n",
    "            verify_threshold=verify_threshold\n",
    "        )\n",
    "\n",
    "# Test adaptive thresholds with fallback demonstration\n",
    "cache = AdaptiveSemanticCache()\n",
    "cache.add(\"What is the annual revenue?\", \"revenue_tool\")\n",
    "cache.add(\"Show me customer demographics\", \"demographics_tool\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"yearly revenue\", \"Strong match\"),\n",
    "    (\"customer demographic\", \"Weaker match\")\n",
    "]\n",
    "\n",
    "for query, description in test_cases:\n",
    "    print(f\"\\nQuery: '{query}' ({description})\")\n",
    "    for match_type in ['exact', 'normal', 'fuzzy']:\n",
    "        result = cache.adaptive_search(query, match_type)\n",
    "        threshold = cache.get_threshold(match_type)\n",
    "        if result:\n",
    "            print(f\"  {match_type.upper()} (threshold {threshold:.2f}): ✅ Found match\")\n",
    "        else:\n",
    "            print(f\"  {match_type.upper()} (threshold {threshold:.2f}): ❌ No match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc0d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with auto-population:\n",
      "'What is my account balance?' → checking_balance_tool (cache)\n",
      "'Show me my account balance' → balance_tool (cache)\n",
      "'Account balance please' → balance_tool (cache)\n",
      "'Recent transactions' → transaction_tool (cache)\n",
      "'Show my transactions' → transaction_tool (cache)\n",
      "\n",
      "Cache Stats:\n",
      "  Hits: 5 (100.0%)\n",
      "  Misses: 0\n",
      "  Auto-added: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Auto-Population and Statistics\n",
    "class SmartSemanticCache(AdaptiveSemanticCache):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', collection_ref=None):\n",
    "        super().__init__(model_name=model_name, collection_ref=collection_ref)\n",
    "        self.stats = {'hits': 0, 'misses': 0, 'auto_added': 0}\n",
    "\n",
    "    def query_with_fallback(self, query, fallback_fn=None, match_type='normal'):\n",
    "        \"\"\"Try cache first, fallback to function if miss\"\"\"\n",
    "        result = self.adaptive_search(query, match_type)\n",
    "        if result:\n",
    "            self.stats['hits'] += 1\n",
    "            return result['response'], 'cache'\n",
    "\n",
    "        self.stats['misses'] += 1\n",
    "        if fallback_fn:\n",
    "            response = fallback_fn(query)\n",
    "            self.add(query, response)\n",
    "            self.stats['auto_added'] += 1\n",
    "            return response, 'computed'\n",
    "\n",
    "        return None, 'miss'\n",
    "\n",
    "    def print_stats(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        if total > 0:\n",
    "            hit_rate = self.stats['hits'] / total * 100\n",
    "            print(\"Cache Stats:\")\n",
    "            print(f\"  Hits: {self.stats['hits']} ({hit_rate:.1f}%)\")\n",
    "            print(f\"  Misses: {self.stats['misses']}\")\n",
    "            print(f\"  Auto-added: {self.stats['auto_added']}\")\n",
    "\n",
    "# Mock agent function\n",
    "def mock_agent(query):\n",
    "    \"\"\"Simulate an expensive agent call\"\"\"\n",
    "    if 'balance' in query.lower():\n",
    "        return 'balance_tool'\n",
    "    elif 'transaction' in query.lower():\n",
    "        return 'transaction_tool'\n",
    "    else:\n",
    "        return 'general_tool'\n",
    "\n",
    "# Test with fallback\n",
    "cache = SmartSemanticCache()\n",
    "queries = [\n",
    "    \"What is my account balance?\",\n",
    "    \"Show me my account balance\",\n",
    "    \"Account balance please\",\n",
    "    \"Recent transactions\",\n",
    "    \"Show my transactions\",\n",
    "]\n",
    "\n",
    "print(\"Testing with auto-population:\")\n",
    "for q in queries:\n",
    "    response, source = cache.query_with_fallback(q, mock_agent)\n",
    "    print(f\"'{q}' → {response} ({source})\")\n",
    "\n",
    "print()\n",
    "cache.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67483fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is my account balance?\n",
      "✓ Cache hit: checking_balance_tool\n",
      "\n",
      "Query: Show me my account balance\n",
      "✓ Cache hit: balance_tool\n",
      "\n",
      "Query: What are my recent transactions?\n",
      "✓ Cache hit: transaction_tool\n",
      "\n",
      "Query: Display my transactions\n",
      "✓ Cache hit: transaction_tool\n",
      "\n",
      "Query: What is my balance?\n",
      "✓ Cache hit: balance_tool\n",
      "\n",
      "Stats: 5 hits, 0 misses\n",
      "Hit rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Step 8 (Bonus step!): Fallback to Planning Agent\n",
    "class CacheWithFallback:\n",
    "    def __init__(self, cache_impl=None):\n",
    "        # Reuse the verified cache (or inject a smarter one)\n",
    "        self.cache = cache_impl if cache_impl is not None else CrossEncoderSemanticCache()\n",
    "        self.stats = {'hits': 0, 'misses': 0}\n",
    "\n",
    "    def process_query(self, query):\n",
    "        \"\"\"Try cache first, then fallback to agent\"\"\"\n",
    "        result = self.cache.search_with_verification(query)\n",
    "        if result:\n",
    "            self.stats['hits'] += 1\n",
    "            print(f\"✓ Cache hit: {result['response']}\")\n",
    "            return {**result, 'from_cache': True}\n",
    "\n",
    "        self.stats['misses'] += 1\n",
    "        print(\"✗ Cache miss - calling planning agent...\")\n",
    "\n",
    "        solution = self.mock_planning_agent(query)\n",
    "\n",
    "        if solution:\n",
    "            self.cache.add(query, solution)\n",
    "            print(f\"→ Added to cache: {solution}\")\n",
    "\n",
    "        return {'solution_path': solution, 'from_cache': False}\n",
    "\n",
    "    def mock_planning_agent(self, query):\n",
    "        \"\"\"Simulate a planning agent (replace with real agent)\"\"\"\n",
    "        q = query.lower()\n",
    "        if 'balance' in q:\n",
    "            return 'balance_tool'\n",
    "        elif 'transaction' in q:\n",
    "            return 'transaction_tool'\n",
    "        elif 'stock' in q:\n",
    "            return 'stock_tool'\n",
    "        else:\n",
    "            return 'general_tool'\n",
    "\n",
    "    def print_stats(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = (self.stats['hits'] / total) if total > 0 else 0\n",
    "        print(f\"\\nStats: {self.stats['hits']} hits, {self.stats['misses']} misses\")\n",
    "        print(f\"Hit rate: {hit_rate:.1%}\")\n",
    "\n",
    "# Test the full system\n",
    "system = CacheWithFallback()\n",
    "\n",
    "queries = [\n",
    "    \"What is my account balance?\",\n",
    "    \"Show me my account balance\",\n",
    "    \"What are my recent transactions?\",\n",
    "    \"Display my transactions\",\n",
    "    \"What is my balance?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    system.process_query(q)\n",
    "\n",
    "system.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89efd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
